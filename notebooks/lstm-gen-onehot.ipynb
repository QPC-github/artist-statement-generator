{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K\n",
    "import os.path\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from artstat import util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'artstat.util' from '../src/artstat/util.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/home/pmilovanov/hg/my/data/gallery-pr/all\"\n",
    "\n",
    "path_train = os.path.join(datadir, \"train\")\n",
    "path_test = os.path.join(datadir, \"test\")\n",
    "\n",
    "glove = \"/home/pmilovanov/data/glove/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocab = util.load_vocab(\"../vocab.txt\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = util.load_embeddings(vocab, 300, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb matrix shape (10001, 300)\n",
      "vocab len 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"emb matrix shape\", emb_matrix.shape)\n",
    "print(\"vocab len\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/22860 [00:00<00:46, 489.87it/s]\n"
     ]
    }
   ],
   "source": [
    "imp.reload(util)\n",
    "\n",
    "seqlen = 128\n",
    "stride = 96\n",
    "X, Y, Xu, Yu = util.load_data_sequences(path_train, vocab, seqlen, stride, numfiles=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = K.Input((seqlen,), dtype=\"int32\", name=\"input_x\")\n",
    "input_xu = K.Input((seqlen,), dtype=\"float32\", name=\"input_xu\")\n",
    "input_y = K.Input((seqlen,), dtype=\"int32\", name=\"input_y\")\n",
    "input_yu = K.Input((seqlen,), dtype=\"float32\", name=\"input_yu\")\n",
    "\n",
    "reshaper_u  = K.layers.Reshape((seqlen,1))\n",
    "resh_xu = reshaper_u(input_xu)\n",
    "resh_yu = reshaper_u(input_yu)\n",
    "\n",
    "dim = emb_matrix.shape[1] + 1\n",
    "\n",
    "L_emb = K.layers.Embedding(emb_matrix.shape[0], \n",
    "                           emb_matrix.shape[1], \n",
    "                           input_length=seqlen, \n",
    "                           trainable=False,\n",
    "                           weights=[emb_matrix],\n",
    "                           name=\"embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemb = L_emb\n",
    "emb_x = Lemb(input_x)\n",
    "#emb_y = Lemb(input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lconcat = K.layers.Concatenate()\n",
    "concat_x = Lconcat([emb_x, resh_xu])\n",
    "#concat_y = Lconcat([emb_y, resh_yu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = K.layers.CuDNNLSTM(256, return_sequences=True)(concat_x)\n",
    "dense = K.layers.BatchNormalization()(lstm)\n",
    "dense = K.layers.Dense(512, activation=\"relu\")(dense)\n",
    "dense = K.layers.BatchNormalization()(dense)\n",
    "dense = K.layers.Dense(len(vocab)+2, activation=\"softmax\")(dense)\n",
    "\n",
    "#dense = K.layers.BatchNormalization()(lstm)\n",
    "#lstm = K.layers.CuDNNLSTM(256, return_sequences=True)(lstm)\n",
    "#dense = K.layers.Dense(dim, activation=\"sigmoid\")(dense)\n",
    "#dense = K.layers.BatchNormalization()(dense)\n",
    "#dense = K.layers.Dense(dim, activation=\"sigmoid\")(dense)\n",
    "#dense = K.layers.BatchNormalization()(dense)\n",
    "#dense = K.layers.Dense(dim, activation=\"sigmoid\")(dense)\n",
    "#dense = K.layers.BatchNormalization()(dense)\n",
    "#dense = K.layers.Dense(dim, activation=\"sigmoid\")(dense)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = K.Model(inputs=[input_x, input_xu],\n",
    "               outputs=[dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = K.optimizers.Adam(lr=0.01)\n",
    "model.compile(opt, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(617, 128)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "def data_generator(X, Xu, Y, Yu, batch_size=batch_size, vocab_size=10000):\n",
    "    batches = int(math.floor(X.shape[0]/batch_size))\n",
    "    while True:\n",
    "        for i in range(batches):\n",
    "            start, end = i*batch_size, (i+1)*batch_size\n",
    "            bX  = X[start:end]\n",
    "            bXu = Xu[start:end]\n",
    "            Yc = K.utils.to_categorical(Y[start:end], num_classes=vocab_size)\n",
    "            bY = np.concatenate((Yc,\n",
    "                            np.expand_dims(Yu[start:end], axis=2)), axis=2)\n",
    "            yield [[bX, bXu], [bY]]\n",
    "        \n",
    "datagen = data_generator(X, Xu, Y, Yu, vocab_size=len(vocab)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "9/9 [==============================] - 5s 573ms/step - loss: 1.6528\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 5s 550ms/step - loss: 1.3529\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 5s 547ms/step - loss: 1.1629\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 5s 542ms/step - loss: 1.0230\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 5s 552ms/step - loss: 0.9420\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 5s 559ms/step - loss: 0.8545\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 5s 550ms/step - loss: 0.7902\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 5s 556ms/step - loss: 0.7018\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 5s 556ms/step - loss: 0.6444\n",
      "Epoch 10/1000\n",
      "5/9 [===============>..............] - ETA: 2s - loss: 0.5804"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-acc3c43713b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m         return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1377\u001b[0;31m             self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "numbatches = int(math.floor(X.shape[0]/batch_size))\n",
    "model.fit_generator(datagen, steps_per_epoch=numbatches, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = K.Model([input_x, input_xu], [dense])\n",
    "predictor.compile(opt, loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n"
     ]
    }
   ],
   "source": [
    "i = 7\n",
    "gen = X[i].tolist()\n",
    "genu = Xu[i].tolist()\n",
    "\n",
    "tX = np.zeros((1, seqlen), dtype=\"int32\")\n",
    "tXu = np.zeros((1, seqlen), dtype=\"float32\")\n",
    "results = []\n",
    "\n",
    "iterations = 128\n",
    "\n",
    "tX[0,:] = np.array(gen[-seqlen:], \"int32\")\n",
    "tXu[0,:] = np.array(genu[-seqlen:], \"float32\")\n",
    "\n",
    "z = predictor.predict([tX, tXu])\n",
    "\n",
    "scores = z[0, -1]\n",
    "\n",
    "idx = np.random.choice(len(vocab)+2, p=scores)\n",
    "\n",
    "print(words[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "i = 7\n",
    "gen = X[i].tolist()\n",
    "genu = Xu[i].tolist()\n",
    "\n",
    "tX = np.zeros((1, seqlen), dtype=\"int32\")\n",
    "tXu = np.zeros((1, seqlen), dtype=\"float32\")\n",
    "results = []\n",
    "\n",
    "iterations = 128\n",
    "for j in range(iterations):\n",
    "    tX[0,:] = np.array(gen[-seqlen:], \"int32\")\n",
    "    tXu[0,:] = np.array(genu[-seqlen:], \"float32\")\n",
    "    #print(tX)\n",
    "    xv, xuv = 0, 0.0\n",
    "    z = predictor.predict([tX, tXu])\n",
    "    if z[0, -1, -1] > 0.1:\n",
    "        xuv = 1.0\n",
    "        results.append(\"<UNK>\")\n",
    "    else:\n",
    "        we = np.transpose(pp.normalize(np.reshape(z[0, -1, :300], (1,300))))\n",
    "#        print(np.linalg.norm(we))\n",
    "#        scores = np.matmul(norm_emb_matrix, we)\n",
    "#        print(scores)\n",
    "#        print(np.min(scores))\n",
    "#        break\n",
    "        xv = np.argmax(np.matmul(norm_emb_matrix, we))\n",
    "        if xv:\n",
    "            results.append(words[xv])\n",
    "        else:\n",
    "            results.append(\"<EMPTY>\")\n",
    "    gen.append(xv)\n",
    "    genu.append(xuv)\n",
    "    print(results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3547268e-16 0.93638355\n"
     ]
    }
   ],
   "source": [
    "print(np.min(z), np.max(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_emb_matrix =  preprocessing.normalize(emb_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "zn = preprocessing.normalize(z[0,:], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = zn[-1, :300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.matmul(norm_emb_matrix, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(scores)\n",
    "print(idx)\n",
    "word = words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
