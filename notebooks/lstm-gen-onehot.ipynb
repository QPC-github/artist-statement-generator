{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "seqlen = 128\n",
    "batch_size = 128\n",
    "sample_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K\n",
    "import os.path\n",
    "\n",
    "import sys, imp\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from artstat import util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/home/pmilovanov/hg/my/data/gallery-pr/all\"\n",
    "\n",
    "path_train = os.path.join(datadir, \"train\")\n",
    "path_test = os.path.join(datadir, \"test\")\n",
    "\n",
    "glove = \"/home/pmilovanov/data/glove/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocab = util.load_vocab(\"../vocab.txt\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = util.load_embeddings(vocab, 300, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22860 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "imp.reload(util)\n",
    "X, Xu = util.load_data(path_train, vocab, pad=seqlen, numfiles=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape, Embedding, CuDNNLSTM, BatchNormalization, Dense, Concatenate\n",
    "from keras import Model, Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(5)\n",
    "b = np.ones_like(a)\n",
    "\n",
    "z = np.concatenate((np.expand_dims(b, axis=-1),\n",
    "                np.expand_dims(a, axis=-1)),-1)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 2)\n",
      "pp Tensor(\"strided_slice_2:0\", shape=(), dtype=int32)\n",
      "(3, 2, 5)\n",
      "[[[  0.   1.   2.   3.   4.]\n",
      "  [ 25.  26.  27.  28.  29.]]\n",
      "\n",
      " [[105. 106. 107. 108. 109.]\n",
      "  [130. 131. 132. 133. 134.]]\n",
      "\n",
      " [[210. 211. 212. 213. 214.]\n",
      "  [235. 236. 237. 238. 239.]]]\n"
     ]
    }
   ],
   "source": [
    "A = np.zeros((3, 10, 5))\n",
    "\n",
    "for i in range(3):\n",
    "    A[i,:,:] = np.reshape(np.array(range(50)), (10,5)) + i*100\n",
    "\n",
    "#print(A)\n",
    "    #A\n",
    "#var = K.variable\n",
    "\n",
    "indices = np.array([\n",
    "    [[0,0], [0,5]],\n",
    "    [[1,1], [1,6]],\n",
    "    [[2,2], [2,7]]\n",
    "], dtype=\"int32\")\n",
    "print(indices.shape)\n",
    "\n",
    "#indices = np.array([0,5\n",
    "#], dtype=\"int32\")\n",
    "\n",
    "v = K.backend.variable(A)\n",
    "k = K.backend.variable(indices, dtype=\"int32\")\n",
    "\n",
    "pp = tf.shape(v)[0]\n",
    "print(\"pp\",pp)\n",
    "\n",
    "g = tf.gather_nd(A, indices)\n",
    "\n",
    "q = K.backend.eval(g)\n",
    "print(q.shape)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(10, 5)\n",
      "(4,)\n",
      "[[[  0.   1.   2.   3.   4.]\n",
      "  [  5.   6.   7.   8.   9.]\n",
      "  [ 10.  11.  12.  13.  14.]\n",
      "  [  0.   1.   2.   3.   4.]]\n",
      "\n",
      " [[105. 106. 107. 108. 109.]\n",
      "  [110. 111. 112. 113. 114.]\n",
      "  [115. 116. 117. 118. 119.]\n",
      "  [105. 106. 107. 108. 109.]]\n",
      "\n",
      " [[210. 211. 212. 213. 214.]\n",
      "  [215. 216. 217. 218. 219.]\n",
      "  [220. 221. 222. 223. 224.]\n",
      "  [210. 211. 212. 213. 214.]]]\n"
     ]
    }
   ],
   "source": [
    "q = np.array([\n",
    "    [0,1,2,0],\n",
    "    [1,2,3,1],\n",
    "    [2,3,4,2]\n",
    "])\n",
    "\n",
    "qv = K.backend.variable(q, dtype=\"int32\")\n",
    "\n",
    "def mygather(e):\n",
    "    batch, indices = e\n",
    "    print(type(batch))\n",
    "    print(batch.shape)\n",
    "    print(indices.shape)\n",
    "    #i2 = tf.expand_dims(indices)\n",
    "    #print(i2.shape)\n",
    "    return (tf.gather(batch, indices),0)\n",
    "\n",
    "r = tf.map_fn(mygather, (v, qv))\n",
    "\n",
    "print(K.backend.eval(r[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_layer(x):\n",
    "    data, sample_indices = x\n",
    "    def gather_per_batch(e):\n",
    "        batch, indices = e\n",
    "        indices = tf.cast(indices, tf.int32)\n",
    "        #print(\"batch\", type(batch), batch.dtype, batch.shape)\n",
    "        #print(\"indices\", type(indices), indices.dtype, indices.shape)\n",
    "        return (tf.gather(batch, indices),0)\n",
    "    return tf.map_fn(gather_per_batch, (data, sample_indices),\n",
    "                    parallel_iterations=batch_size*2, swap_memory=False)[0]\n",
    "\n",
    "\n",
    "def sampling_layer_gather_nd(x):\n",
    "    data, sample_indices = x\n",
    "    return tf.gather_nd(data, tf.cast(sample_indices, tf.int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,   1.,   2.,   3.,   4.],\n",
       "        [  5.,   6.,   7.,   8.,   9.],\n",
       "        [ 10.,  11.,  12.,  13.,  14.],\n",
       "        [  0.,   1.,   2.,   3.,   4.]],\n",
       "\n",
       "       [[105., 106., 107., 108., 109.],\n",
       "        [110., 111., 112., 113., 114.],\n",
       "        [115., 116., 117., 118., 119.],\n",
       "        [105., 106., 107., 108., 109.]],\n",
       "\n",
       "       [[210., 211., 212., 213., 214.],\n",
       "        [215., 216., 217., 218., 219.],\n",
       "        [220., 221., 222., 223., 224.],\n",
       "        [210., 211., 212., 213., 214.]]], dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = sampling_layer([v, qv])\n",
    "\n",
    "K.backend.eval(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape, Embedding, CuDNNLSTM, BatchNormalization, Dense, \\\n",
    "    Concatenate, Lambda, Activation\n",
    "from keras import Model, Input\n",
    "\n",
    "def make_model(*, seqlen, sample_size, emb_matrix):    \n",
    "    dim = emb_matrix.shape[1] + 1\n",
    "    \n",
    "    input_x = Input((seqlen,), dtype=\"int32\", name=\"input_x\")\n",
    "    input_xu = Input((seqlen,), dtype=\"float32\", name=\"input_xu\")\n",
    "    input_sample_indices = Input((sample_size,2), dtype=\"int32\", name=\"input_sample_indices\")\n",
    "\n",
    "    resh_xu  = Reshape((seqlen,1), name=\"resh_xu\")(input_xu)\n",
    "\n",
    "    emb_layer = Embedding(*emb_matrix.shape, input_length=seqlen, \n",
    "                          trainable=False, weights=[emb_matrix], \n",
    "                          name=\"embedding\")\n",
    "    emb_x = emb_layer(input_x)\n",
    "    concat_x = Concatenate(name=\"concat_x\")([emb_x, resh_xu])\n",
    "        \n",
    "    yhat = CuDNNLSTM(128, return_sequences=True, name='rnn1')(concat_x)\n",
    "    yhat = BatchNormalization()(yhat)\n",
    "    yhat = CuDNNLSTM(128, return_sequences=False, name='rnn2')(yhat)\n",
    "\n",
    "    yhat = BatchNormalization()(yhat)\n",
    "    yhat = Dense(300, activation=\"relu\")(yhat)\n",
    "    yhat = BatchNormalization()(yhat)\n",
    "    # len(vocab)+2 is because the zeroth word is for padding and last word is for \"unknown\"\n",
    "    yhat = Dense(len(vocab)+2, activation=\"linear\")(yhat)\n",
    "    \n",
    "    \n",
    "    #print(input_sample_indices.dtype, input_sample_indices.shape)\n",
    "    out_train = Lambda(sampling_layer_gather_nd, name=\"sampling\")([yhat, input_sample_indices])\n",
    "    out_train = Activation('softmax')(out_train)\n",
    "    \n",
    "    out_predict = Activation('softmax')(yhat)\n",
    "    \n",
    "    model_train = Model([input_x, input_xu, input_sample_indices], [out_train])\n",
    "    model_predict = Model([input_x, input_xu], [out_predict])\n",
    "    \n",
    "    return model_train, model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrain, mpredict = make_model(seqlen=seqlen, sample_size=sample_size, emb_matrix=emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_x (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_xu (InputLayer)           (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 128, 300)     3000300     input_x[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "resh_xu (Reshape)               (None, 128, 1)       0           input_xu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_x (Concatenate)          (None, 128, 301)     0           embedding[0][0]                  \n",
      "                                                                 resh_xu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "rnn1 (CuDNNLSTM)                (None, 128, 128)     220672      concat_x[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 128, 128)     512         rnn1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "rnn2 (CuDNNLSTM)                (None, 128)          132096      batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 128)          512         rnn2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 300)          38700       batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 300)          1200        dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 10002)        3010602     batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_sample_indices (InputLaye (None, 5, 2)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Lambda)               (None, 5)            0           dense_48[0][0]                   \n",
      "                                                                 input_sample_indices[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5)            0           sampling[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,404,594\n",
      "Trainable params: 3,403,182\n",
      "Non-trainable params: 3,001,412\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mtrain.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = K.optimizers.Adam(lr=0.01)\n",
    "mtrain.compile(opt, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(util)\n",
    "train_seq = util.NegativeSamplingPermutedSequence(data_x=X, data_xu=Xu,\n",
    "                                                 seqlen=seqlen, batch_size=batch_size,\n",
    "                                                 sample_size=sample_size,\n",
    "                                                 vocab_size=len(vocab)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21201 16384\n"
     ]
    }
   ],
   "source": [
    "print(len(X), seqlen*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50000\n",
      "165/165 [==============================] - 13s 76ms/step - loss: 0.1139\n",
      "Epoch 2/50000\n",
      " 50/165 [========>.....................] - ETA: 6s - loss: 0.1319"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-487d3714a973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnumbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m mtrain.fit_generator(train_seq, steps_per_epoch=numbatches, epochs=50000\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m         return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1377\u001b[0;31m             self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "numbatches = X.shape[0] // batch_size\n",
    "mtrain.fit_generator(train_seq, steps_per_epoch=numbatches, epochs=50000\n",
    "                   \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpredict.compile(opt, loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long tragic conceptual selection way . interdisciplinary result film large programme multi international three place new film recent long multi global tour historical selection new spirit new new artist additional narrow new "
     ]
    }
   ],
   "source": [
    "i = 2999\n",
    "gen = X[i:i+seqlen].tolist()\n",
    "genu = Xu[i:i+seqlen].tolist()\n",
    "\n",
    "tX = np.zeros((1, seqlen), dtype=\"int32\")\n",
    "tXu = np.zeros((1, seqlen), dtype=\"float32\")\n",
    "results = []\n",
    "\n",
    "UNK_IDX = len(words)\n",
    "\n",
    "iterations = 32\n",
    "for j in range(iterations):\n",
    "    tX[0,:] = np.array(gen[-seqlen:], \"int32\")\n",
    "    tXu[0,:] = np.array(genu[-seqlen:], \"float32\")\n",
    "    \n",
    "    #print(tX)\n",
    "    z = mpredict.predict([tX, tXu])\n",
    " #   scores=z[0]\n",
    "    idx = UNK_IDX\n",
    "    #print(scores)\n",
    "    while idx == UNK_IDX:\n",
    "        idx = np.random.choice(range(len(vocab)+2), p=scores)\n",
    "\n",
    "    gen.append(idx)\n",
    "    genu.append(0.0)\n",
    "    #print(\"idx\", idx, UNK_IDX)\n",
    "    results.append(words[idx])\n",
    "    sys.stdout.write(results[-1] + \" \")\n",
    "    sys.stdout.flush()\n",
    "#    print(\"%s \", results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3547268e-16 0.93638355\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_emb_matrix =  preprocessing.normalize(emb_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "zn = preprocessing.normalize(z[0,:], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = zn[-1, :300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.matmul(norm_emb_matrix, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(scores)\n",
    "print(idx)\n",
    "word = words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
