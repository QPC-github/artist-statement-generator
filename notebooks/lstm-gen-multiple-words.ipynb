{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K\n",
    "import os.path\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from artstat import util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'artstat.util' from '../src/artstat/util.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/home/pmilovanov/hg/my/data/gallery-pr/all\"\n",
    "\n",
    "path_train = os.path.join(datadir, \"train\")\n",
    "path_test = os.path.join(datadir, \"test\")\n",
    "\n",
    "glove = \"/home/pmilovanov/data/glove/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocab = util.load_vocab(\"../vocab.txt\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = util.load_embeddings(vocab, 300, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 300) 100000\n"
     ]
    }
   ],
   "source": [
    "print(emb_matrix.shape, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 22860/22860 [00:35<00:00, 652.87it/s]\n"
     ]
    }
   ],
   "source": [
    "imp.reload(util)\n",
    "\n",
    "seqlen = 128\n",
    "stride = 96\n",
    "X, Y, Xu, Yu = util.load_data_sequences(path_train, vocab, seqlen, stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yfake = np.zeros((Yu.shape[0], 128), dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, Reshape, Concatenate, CuDNNLSTM, Lambda\n",
    "import keras as K\n",
    "from keras import backend as B\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = K.Input((seqlen,), dtype=\"int32\", name=\"input_x\")\n",
    "input_xu = K.Input((seqlen,), dtype=\"float32\", name=\"input_xu\")\n",
    "input_y = K.Input((seqlen,), dtype=\"int32\", name=\"input_y\")\n",
    "input_yu = K.Input((seqlen,), dtype=\"float32\", name=\"input_yu\")\n",
    "\n",
    "reshaper_u  = K.layers.Reshape((seqlen,1))\n",
    "resh_xu = reshaper_u(input_xu)\n",
    "resh_yu = reshaper_u(input_yu)\n",
    "\n",
    "dim = emb_matrix.shape[1] + 1\n",
    "\n",
    "L_emb = K.layers.Embedding(emb_matrix.shape[0], \n",
    "                           emb_matrix.shape[1], \n",
    "                           input_length=seqlen, \n",
    "                          trainable=False,\n",
    "                           weights=[emb_matrix],\n",
    "                           name=\"embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemb = L_emb\n",
    "emb_x = Lemb(input_x)\n",
    "emb_y = Lemb(input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lconcat = K.layers.Concatenate()\n",
    "concat_x = Lconcat([emb_x, resh_xu])\n",
    "concat_y = Lconcat([emb_y, resh_yu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def realdot(a,b):\n",
    "    d = K.layers.multiply([a, b])\n",
    "    return B.sum(d, axis=-1)\n",
    "\n",
    "def dotlayer(x):\n",
    "    a,b = x\n",
    "    return realdot(a,b)\n",
    "\n",
    "def cossim(x):\n",
    "    a,b = x\n",
    "    p = realdot(a,b)\n",
    "    an = B.sqrt(realdot(a,a) + B.epsilon())\n",
    "    bn = B.sqrt(realdot(b,b) + B.epsilon())\n",
    "    return tf.divide(p, K.layers.multiply([an, bn]) + B.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 3)\n",
      "(?, 128, 1)\n",
      "(?, 128, 1159)\n",
      "(?, 128, 3)\n",
      "(?, 128)\n",
      "(?, 128)\n"
     ]
    }
   ],
   "source": [
    "lstm =  K.layers.CuDNNLSTM(256, return_sequences=True, name='lstm1')(concat_x)\n",
    "lstm = K.layers.BatchNormalization()(lstm)\n",
    "lstm =  K.layers.CuDNNLSTM(256, return_sequences=True, name='lstm2')(lstm)\n",
    "#lstm = K.layers.CuDNNLSTM(256, return_sequences=True)(lstm)\n",
    "dense = K.layers.BatchNormalization()(lstm)\n",
    "\n",
    "def candidate_gen(inp, layername):\n",
    "    out = K.layers.Dense(256, activation=\"relu\", name=layername+\"_1\")(inp)\n",
    "    out = K.layers.BatchNormalization()(out)\n",
    "    out = K.layers.Dense(256, activation=\"relu\", name=layername+\"_2\")(out)\n",
    "    out = K.layers.BatchNormalization()(out)\n",
    "    out = K.layers.Dense(dim, activation=\"sigmoid\", name=layername+\"_last\")(out)\n",
    "    return out\n",
    "\n",
    "out1 = candidate_gen(dense, \"out1\")\n",
    "out2 = candidate_gen(dense, \"out2\")\n",
    "out3 = candidate_gen(dense, \"out3\")\n",
    "\n",
    "allout = K.layers.Concatenate(name=\"concat_out\")([out1, out2, out3, lstm])\n",
    "probs = K.layers.Dense(256, activation=\"relu\", name=\"probabilities_intermediate\")(allout)\n",
    "probs = K.layers.BatchNormalization()(probs)\n",
    "probs = K.layers.Dense(3, activation=\"softmax\", name=\"probabilities\")(probs)\n",
    "\n",
    "def fexpander(x):\n",
    "    return B.expand_dims(x)\n",
    "L_expander = Lambda(fexpander, name=\"expander\")\n",
    "\n",
    "L_cossim = Lambda(cossim, name=\"cosine_similarity\")\n",
    "prox1 = L_cossim([concat_y, out1])\n",
    "prox1 = L_expander(prox1)\n",
    "prox2 = L_cossim([concat_y, out2])\n",
    "prox2 = L_expander(prox2)\n",
    "prox3 = L_cossim([concat_y, out3])\n",
    "prox3 = L_expander(prox3)\n",
    "\n",
    "allprox = Concatenate(name=\"concat_prox\")([prox1, prox2, prox3])\n",
    "final1 = K.layers.Lambda(dotlayer, name=\"final1\")([probs, allprox])\n",
    "\n",
    "#allones = B.ones(final1.shape, dtype=\"float32\")\n",
    "def final2_fun(x):\n",
    "    return B.abs(1.0 - x)\n",
    "\n",
    "final2 = Lambda(final2_fun, name=\"final2\")(final1)\n",
    "print(probs.shape)\n",
    "print(prox3.shape)\n",
    "print(allout.shape)\n",
    "print(allprox.shape)\n",
    "print(final1.shape)\n",
    "print(final2.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = K.Model(inputs=[input_x, input_xu, input_y, input_yu],\n",
    "               outputs=[final2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_x (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_xu (InputLayer)           (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 128, 300)     30000300    input_x[0][0]                    \n",
      "                                                                 input_y[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 128, 1)       0           input_xu[0][0]                   \n",
      "                                                                 input_yu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 301)     0           embedding[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "                                                                 reshape_1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (CuDNNLSTM)               (None, 128, 256)     572416      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 128, 256)     1024        lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (CuDNNLSTM)               (None, 128, 256)     526336      batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 128, 256)     1024        lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out1_1 (Dense)                  (None, 128, 256)     65792       batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "out2_1 (Dense)                  (None, 128, 256)     65792       batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "out3_1 (Dense)                  (None, 128, 256)     65792       batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 128, 256)     1024        out1_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 128, 256)     1024        out2_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 128, 256)     1024        out3_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out1_2 (Dense)                  (None, 128, 256)     65792       batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "out2_2 (Dense)                  (None, 128, 256)     65792       batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "out3_2 (Dense)                  (None, 128, 256)     65792       batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 128, 256)     1024        out1_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 128, 256)     1024        out2_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 128, 256)     1024        out3_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_y (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_yu (InputLayer)           (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "out1_last (Dense)               (None, 128, 301)     77357       batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "out2_last (Dense)               (None, 128, 301)     77357       batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "out3_last (Dense)               (None, 128, 301)     77357       batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concat_out (Concatenate)        (None, 128, 1159)    0           out1_last[0][0]                  \n",
      "                                                                 out2_last[0][0]                  \n",
      "                                                                 out3_last[0][0]                  \n",
      "                                                                 lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "probabilities_intermediate (Den (None, 128, 256)     296960      concat_out[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cosine_similarity (Lambda)      (None, 128)          0           concatenate_1[1][0]              \n",
      "                                                                 out1_last[0][0]                  \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 out2_last[0][0]                  \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 out3_last[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 128, 256)     1024        probabilities_intermediate[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expander (Lambda)               (None, 128, 1)       0           cosine_similarity[0][0]          \n",
      "                                                                 cosine_similarity[1][0]          \n",
      "                                                                 cosine_similarity[2][0]          \n",
      "__________________________________________________________________________________________________\n",
      "probabilities (Dense)           (None, 128, 3)       771         batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concat_prox (Concatenate)       (None, 128, 3)       0           expander[0][0]                   \n",
      "                                                                 expander[1][0]                   \n",
      "                                                                 expander[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "final1 (Lambda)                 (None, 128)          0           probabilities[0][0]              \n",
      "                                                                 concat_prox[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "final2 (Lambda)                 (None, 128)          0           final1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 32,032,822\n",
      "Trainable params: 2,027,914\n",
      "Non-trainable params: 30,004,908\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = K.optimizers.Adam(lr=0.01)\n",
    "model.compile(opt, loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000000\n",
      " 31744/157646 [=====>........................] - ETA: 1:04 - loss: 0.5561"
     ]
    }
   ],
   "source": [
    "model.fit([X, Xu, Y, Yu], [Yfake], batch_size=1024, epochs=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = K.Model([input_x, input_xu], [out1, out2, out3, probs])\n",
    "predictor.compile(opt, loss=\"mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "norm_emb_matrix =  pp.normalize(Lemb.get_weights()[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157646"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0.0), ('registrations', 1.0), ('see', 1.2173728e-19)]\n",
      "registrations\n",
      "[('as', 2.4678491e-15), (',', 1.0), ('the', 2.4923575e-11)]\n",
      ",\n",
      "[('that', 0.0), ('and', 1.0), ('see', 4.6645018e-24)]\n",
      "and\n",
      "[('that', 0.9999981), ('or', 1.2818558e-06), ('see', 5.48748e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('30', 7.297992e-07), ('see', 5.628823e-07)]\n",
      "that\n",
      "[('that', 0.9999975), ('and', 2.152433e-06), ('see', 3.671109e-07)]\n",
      "that\n",
      "[('that', 0.99999917), ('and', 4.184265e-07), ('see', 3.151159e-07)]\n",
      "that\n",
      "[('that', 0.9999989), ('as', 6.550726e-07), ('see', 4.4266216e-07)]\n",
      "that\n",
      "[('that', 0.9999993), ('as', 4.1415322e-07), ('see', 3.79203e-07)]\n",
      "that\n",
      "[('that', 0.99984264), ('as', 0.000150216), ('see', 7.0977485e-06)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 2.1378512e-20)]\n",
      "as\n",
      "[('that', 0.99999833), ('during', 9.854743e-07), ('see', 6.876204e-07)]\n",
      "that\n",
      "[('that', 0.9999962), ('and', 3.1465715e-06), ('see', 6.9511685e-07)]\n",
      "that\n",
      "[('that', 0.99999654), ('or', 2.7956905e-06), ('see', 6.943386e-07)]\n",
      "that\n",
      "[('that', 0.9999975), ('or', 2.085553e-06), ('see', 5.3069056e-07)]\n",
      "that\n",
      "[('that', 0.99999785), ('or', 1.670094e-06), ('see', 4.3926755e-07)]\n",
      "that\n",
      "[('that', 0.999998), ('or', 1.3787761e-06), ('see', 6.312316e-07)]\n",
      "that\n",
      "[('that', 0.9999993), ('or', 3.5377337e-07), ('see', 3.7650184e-07)]\n",
      "that\n",
      "[('that', 0.9999962), ('or', 3.1465715e-06), ('see', 6.9511685e-07)]\n",
      "that\n",
      "[('that', 0.9999981), ('or', 1.2818558e-06), ('see', 5.48748e-07)]\n",
      "that\n",
      "[('that', 0.9999939), ('as', 4.9815917e-06), ('see', 1.1036993e-06)]\n",
      "that\n",
      "[('that', 0.00016297345), ('as', 0.99983466), ('see', 2.4157157e-06)]\n",
      "as\n",
      "[('that', 0.0), ('during', 1.0), ('see', 8.22219e-18)]\n",
      "during\n",
      "[('the', 0.9999969), ('during', 2.325288e-06), ('see', 6.5655536e-07)]\n",
      "the\n",
      "[('that', 0.9999993), ('MIR', 3.981528e-07), ('see', 3.427529e-07)]\n",
      "that\n",
      "[('that', 0.99999666), ('and', 2.514391e-06), ('see', 7.7682415e-07)]\n",
      "that\n",
      "[('that', 0.99999905), ('or', 7.3650676e-07), ('see', 2.6873386e-07)]\n",
      "that\n",
      "[('that', 3.9267858e-14), ('as', 1.0), ('see', 8.0564395e-11)]\n",
      "as\n",
      "[('that', 1.4040571e-13), ('during', 1.0), ('see', 1.3840379e-10)]\n",
      "during\n",
      "[('the', 0.0), ('during', 1.0), ('see', 6.4585232e-18)]\n",
      "during\n",
      "[('the', 0.9999987), ('30', 7.297992e-07), ('see', 5.628823e-07)]\n",
      "the\n",
      "[('past', 0.999998), ('MIR', 1.1532878e-06), ('see', 8.371381e-07)]\n",
      "past\n",
      "[('as', 0.9999846), (',', 1.46735465e-05), ('the', 7.4125103e-07)]\n",
      "as\n",
      "[('that', 0.99999905), ('30', 7.3650676e-07), ('the', 2.6873386e-07)]\n",
      "that\n",
      "[('that', 0.9999938), ('and', 3.8677717e-06), ('see', 2.3525604e-06)]\n",
      "that\n",
      "[('that', 0.9999949), ('as', 4.295474e-06), ('see', 8.3266633e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 6.4585232e-18)]\n",
      "as\n",
      "[('that', 0.99994946), ('during', 4.726328e-05), ('see', 3.3599729e-06)]\n",
      "that\n",
      "[('that', 0.9999987), ('and', 9.2475926e-07), ('see', 3.910762e-07)]\n",
      "that\n",
      "[('that', 0.9999968), ('as', 2.619098e-06), ('see', 6.0983984e-07)]\n",
      "that\n",
      "[('that', 0.99998975), ('as', 8.565212e-06), ('see', 1.6813162e-06)]\n",
      "that\n",
      "[('that', 0.99999905), ('as', 4.7906263e-07), ('see', 4.3724307e-07)]\n",
      "that\n",
      "[('that', 0.9999695), ('as', 2.8996523e-05), ('see', 1.5057155e-06)]\n",
      "that\n",
      "[('that', 0.99999833), ('as', 1.3099676e-06), ('see', 3.4557365e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('as', 7.297992e-07), ('see', 5.628823e-07)]\n",
      "that\n",
      "[('that', 0.9999937), ('as', 5.850049e-06), ('see', 4.6825681e-07)]\n",
      "that\n",
      "[('that', 0.99999905), ('as', 7.3650676e-07), ('see', 2.6873386e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 2.663004e-21)]\n",
      "as\n",
      "[('that', 0.0), ('during', 1.0), ('see', 6.4585232e-18)]\n",
      "during\n",
      "[('the', 1.7351994e-06), ('during', 0.999998), ('see', 2.190246e-07)]\n",
      "during\n",
      "[('the', 0.999998), ('during', 1.3787761e-06), ('see', 6.312316e-07)]\n",
      "the\n",
      "[('past', 0.99999785), ('MIR', 1.7780668e-06), ('see', 3.441822e-07)]\n",
      "past\n",
      "[('as', 0.999997), (',', 2.147763e-06), ('the', 8.1109584e-07)]\n",
      "as\n",
      "[('that', 0.9999958), ('30', 3.711461e-06), ('the', 4.2600243e-07)]\n",
      "that\n",
      "[('that', 0.9999962), ('and', 2.9541748e-06), ('see', 8.74103e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 6.4585232e-18)]\n",
      "as\n",
      "[('that', 0.9999988), ('during', 6.3255135e-07), ('see', 5.7543224e-07)]\n",
      "that\n",
      "[('that', 0.99999785), ('and', 1.670094e-06), ('see', 4.3926755e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('as', 9.2475926e-07), ('see', 3.910762e-07)]\n",
      "that\n",
      "[('that', 0.99999833), ('as', 1.2054506e-06), ('see', 4.5985985e-07)]\n",
      "that\n",
      "[('that', 0.99999857), ('as', 9.2612447e-07), ('see', 4.743172e-07)]\n",
      "that\n",
      "[('that', 0.9999976), ('as', 1.7769476e-06), ('see', 5.469327e-07)]\n",
      "that\n",
      "[('that', 0.999997), ('as', 2.2019187e-06), ('see', 8.188048e-07)]\n",
      "that\n",
      "[('that', 0.99994946), ('as', 4.726328e-05), ('see', 3.3599729e-06)]\n",
      "that\n",
      "[('that', 0.99999666), ('as', 2.514391e-06), ('see', 7.7682415e-07)]\n",
      "that\n",
      "[('that', 0.99999833), ('as', 1.3099676e-06), ('see', 3.4557365e-07)]\n",
      "that\n",
      "[('that', 0.9999993), ('as', 3.981528e-07), ('see', 3.427529e-07)]\n",
      "that\n",
      "[('that', 0.99999857), ('as', 8.7504765e-07), ('see', 5.503289e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 6.4585232e-18)]\n",
      "as\n",
      "[('that', 0.99999714), ('during', 2.2146899e-06), ('see', 6.1588173e-07)]\n",
      "that\n",
      "[('that', 0.9999951), ('as', 3.5131472e-06), ('see', 1.4484646e-06)]\n",
      "that\n",
      "[('that', 0.99999666), ('as', 2.5142665e-06), ('see', 7.826205e-07)]\n",
      "that\n",
      "[('that', 0.9999802), ('as', 1.831076e-05), ('see', 1.4841868e-06)]\n",
      "that\n",
      "[('that', 0.999998), ('as', 1.3787761e-06), ('see', 6.312316e-07)]\n",
      "that\n",
      "[('that', 0.9999939), ('as', 4.9698433e-06), ('see', 1.0142536e-06)]\n",
      "that\n",
      "[('that', 0.9999958), ('as', 3.285649e-06), ('see', 7.9820103e-07)]\n",
      "that\n",
      "[('that', 0.99999475), ('as', 4.039047e-06), ('see', 1.134177e-06)]\n",
      "that\n",
      "[('that', 0.99999833), ('as', 1.3099676e-06), ('see', 3.4557365e-07)]\n",
      "that\n",
      "[('that', 0.9999969), ('as', 2.0727489e-06), ('see', 1.0173566e-06)]\n",
      "that\n",
      "[('that', 0.99998915), ('as', 9.465372e-06), ('see', 1.4300963e-06)]\n",
      "that\n",
      "[('that', 0.99994946), ('as', 4.726328e-05), ('see', 3.3599729e-06)]\n",
      "that\n",
      "[('that', 0.99999666), ('as', 2.514391e-06), ('see', 7.7682415e-07)]\n",
      "that\n",
      "[('that', 0.9999975), ('as', 2.085553e-06), ('see', 5.3069056e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('as', 7.297992e-07), ('see', 5.628823e-07)]\n",
      "that\n",
      "[('that', 0.9999958), ('as', 3.711461e-06), ('see', 4.2600243e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 6.4585232e-18)]\n",
      "as\n",
      "[('that', 0.9999987), ('during', 7.297992e-07), ('see', 5.628823e-07)]\n",
      "that\n",
      "[('that', 0.9999995), ('as', 3.0107745e-07), ('see', 1.5068115e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('as', 9.2475926e-07), ('see', 3.910762e-07)]\n",
      "that\n",
      "[('that', 0.9999974), ('as', 1.8647999e-06), ('see', 6.5568094e-07)]\n",
      "that\n",
      "[('that', 0.99998164), ('as', 1.5705615e-05), ('see', 2.5857744e-06)]\n",
      "that\n",
      "[('that', 0.9999993), ('as', 3.981528e-07), ('see', 3.427529e-07)]\n",
      "that\n",
      "[('that', 0.99999785), ('as', 1.3224173e-06), ('see', 8.4088504e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 4.3778697e-23)]\n",
      "as\n",
      "[('that', 0.9999987), ('during', 9.229434e-07), ('see', 3.28892e-07)]\n",
      "that\n",
      "[('that', 0.999998), ('as', 1.3711369e-06), ('see', 5.9455584e-07)]\n",
      "that\n",
      "[('that', 0.9999981), ('as', 1.2818558e-06), ('see', 5.48748e-07)]\n",
      "that\n",
      "[('that', 0.9999466), ('as', 4.7055586e-05), ('see', 6.287963e-06)]\n",
      "that\n",
      "[('that', 0.999998), ('as', 1.3787761e-06), ('see', 6.312316e-07)]\n",
      "that\n",
      "[('that', 0.9999981), ('as', 1.3756464e-06), ('see', 4.6500983e-07)]\n",
      "that\n",
      "[('that', 0.99999404), ('as', 4.7054564e-06), ('see', 1.3165283e-06)]\n",
      "that\n",
      "[('that', 0.99998164), ('as', 1.5705615e-05), ('see', 2.5857744e-06)]\n",
      "that\n",
      "[('that', 0.99999833), ('as', 1.0534311e-06), ('see', 5.9227983e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 4.4365328e-20)]\n",
      "as\n",
      "[('that', 0.9999981), ('during', 1.238804e-06), ('see', 6.883813e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('as', 9.229434e-07), ('see', 3.28892e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 6.4585232e-18)]\n",
      "as\n",
      "[('that', 0.9999987), ('during', 7.297992e-07), ('see', 5.628823e-07)]\n",
      "that\n",
      "[('that', 0.9999995), ('as', 3.0107745e-07), ('see', 1.5068115e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('as', 9.2475926e-07), ('see', 3.910762e-07)]\n",
      "that\n",
      "[('that', 0.99999654), ('as', 2.563455e-06), ('see', 8.323168e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('as', 7.297992e-07), ('see', 5.628823e-07)]\n",
      "that\n",
      "[('that', 0.999997), ('as', 2.147763e-06), ('see', 8.1109584e-07)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 2.1378512e-20)]\n",
      "as\n",
      "[('that', 0.9999958), ('during', 3.711461e-06), ('see', 4.2600243e-07)]\n",
      "that\n",
      "[('that', 0.9999962), ('as', 3.1465715e-06), ('see', 6.9511685e-07)]\n",
      "that\n",
      "[('that', 0.9999976), ('as', 1.7769476e-06), ('see', 5.469327e-07)]\n",
      "that\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('that', 0.9999968), ('as', 2.2640047e-06), ('see', 9.542453e-07)]\n",
      "that\n",
      "[('that', 0.9999993), ('as', 3.981528e-07), ('see', 3.427529e-07)]\n",
      "that\n",
      "[('that', 0.999995), ('as', 3.635814e-06), ('see', 1.4607718e-06)]\n",
      "that\n",
      "[('that', 0.9999939), ('as', 4.976207e-06), ('see', 1.0990798e-06)]\n",
      "that\n",
      "[('that', 0.99999905), ('as', 7.3650676e-07), ('see', 2.6873386e-07)]\n",
      "that\n",
      "[('that', 0.9999987), ('as', 7.297992e-07), ('see', 5.628823e-07)]\n",
      "that\n",
      "[('that', 0.99984264), ('as', 0.000150216), ('see', 7.0977485e-06)]\n",
      "that\n",
      "[('that', 0.0), ('as', 1.0), ('see', 2.1378512e-20)]\n",
      "as\n",
      "[('that', 0.99999833), ('during', 9.854743e-07), ('see', 6.876204e-07)]\n",
      "that\n",
      "[('that', 0.9999802), ('as', 1.831076e-05), ('see', 1.4841868e-06)]\n",
      "that\n",
      "[('that', 0.99994946), ('as', 4.726328e-05), ('see', 3.3599729e-06)]\n",
      "that\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "gen = X[i].tolist()\n",
    "genu = Xu[i].tolist()\n",
    "\n",
    "tX = np.zeros((1, seqlen), dtype=\"int32\")\n",
    "tXu = np.zeros((1, seqlen), dtype=\"float32\")\n",
    "results = []\n",
    "\n",
    "def vec2word(x):\n",
    "    we = np.transpose(pp.normalize(np.reshape(x[0, -1, :300], (1,300))))\n",
    "    idx = np.argmax(np.matmul(norm_emb_matrix, we))\n",
    "    return idx\n",
    "\n",
    "iterations = 128\n",
    "for j in range(iterations):\n",
    "    tX[0,:] = np.array(gen[-seqlen:], \"int32\")\n",
    "    tXu[0,:] = np.array(genu[-seqlen:], \"float32\")\n",
    "    #print(tX)\n",
    "    xv, xuv = 0, 0.0\n",
    "    yhat = predictor.predict([tX, tXu])\n",
    "    \n",
    "    #for qq in yhat:\n",
    "    #    print(qq.shape)\n",
    "    #print(yhat)\n",
    "    #break\n",
    "    vecid = np.random.choice(3, p=yhat[3][0,0])\n",
    "    z = yhat[vecid]\n",
    "    \n",
    "    candidates=[]\n",
    "    for u,c in enumerate(yhat[:3]):\n",
    "        cid = vec2word(c)\n",
    "        candidates.append((words[cid], yhat[3][0,0,u]))\n",
    "    print(candidates)\n",
    "    \n",
    "    if False: #z[0, -1, -1] > 0.9:\n",
    "        xuv = 1.0\n",
    "        results.append(\"<UNK>\")\n",
    "    else:\n",
    "        we = np.transpose(pp.normalize(np.reshape(z[0, -1, :300], (1,300))))\n",
    "#        print(np.linalg.norm(we))\n",
    "#        scores = np.matmul(norm_emb_matrix, we)\n",
    "#        print(scores)\n",
    "#        print(np.min(scores))\n",
    "#        break\n",
    "        xv = np.argmax(np.matmul(norm_emb_matrix, we))\n",
    "        if xv:\n",
    "            results.append(words[xv])\n",
    "        else:\n",
    "            results.append(\"<EMPTY>\")\n",
    "    gen.append(xv)\n",
    "    genu.append(xuv)\n",
    "    print(results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3547268e-16 0.93638355\n"
     ]
    }
   ],
   "source": [
    "print(np.min(z), np.max(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_emb_matrix =  preprocessing.normalize(emb_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "zn = preprocessing.normalize(z[0,:], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = zn[-1, :300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.matmul(norm_emb_matrix, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(scores)\n",
    "print(idx)\n",
    "word = words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
