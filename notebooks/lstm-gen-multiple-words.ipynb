{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K\n",
    "import os.path\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from artstat import util\n",
    "import numpy as np\n",
    "\n",
    "floatx = \"float32\"\n",
    "K.backend.set_floatx(floatx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'artstat.util' from '../src/artstat/util.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/home/pmilovanov/hg/my/data/gallery-pr/all\"\n",
    "\n",
    "path_train = os.path.join(datadir, \"train\")\n",
    "path_test = os.path.join(datadir, \"test\")\n",
    "\n",
    "glove = \"/home/pmilovanov/data/glove/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocab = util.load_vocab(\"../vocab.txt\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = util.load_embeddings(vocab, 300, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez(\"embmat_100K.npz\", emb_matrix=emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 300) 10000\n"
     ]
    }
   ],
   "source": [
    "print(emb_matrix.shape, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 22860/22860 [00:35<00:00, 652.49it/s]\n"
     ]
    }
   ],
   "source": [
    "imp.reload(util)\n",
    "\n",
    "seqlen = 128\n",
    "stride = 128\n",
    "X, Y, Xu, Yu = util.load_data_sequences(path_train, vocab, seqlen, stride)\n",
    "Yfake = np.zeros((Yu.shape[0], seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yfake = np.zeros((Yu.shape[0], seqlen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, Reshape, Concatenate, CuDNNLSTM, Lambda\n",
    "import keras as K\n",
    "from keras import backend as B\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6406cf9f03e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mreshaper_u\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresh_xu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshaper_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_xu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresh_yu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshaper_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_yu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             if all([s is not None\n\u001b[1;32m    479\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# input shape known? then we can compute the output shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             return (input_shape[0],) + self._fix_unknown_dimension(\n\u001b[0;32m--> 404\u001b[0;31m                 input_shape[1:], self.target_shape)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m_fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munknown\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "input_x = K.Input((seqlen,), dtype=\"int32\", name=\"input_x\")\n",
    "input_xu = K.Input((seqlen,), dtype=floatx, name=\"input_xu\")\n",
    "input_y = K.Input((seqlen,), dtype=\"int32\", name=\"input_y\")\n",
    "input_yu = K.Input((seqlen,), dtype=floatx, name=\"input_yu\")\n",
    "input_y = K.Input((seqlen,), dtype=\"int32\", name=\"input_y\")\n",
    "input_yu = K.Input((seqlen,), dtype=floatx, name=\"input_yu\")\n",
    "\n",
    "\n",
    "reshaper_u  = K.layers.Reshape((seqlen,1))\n",
    "resh_xu = reshaper_u(input_xu)\n",
    "resh_yu = reshaper_u(input_yu)\n",
    "\n",
    "dim = emb_matrix.shape[1] + 1\n",
    "\n",
    "L_emb = K.layers.Embedding(emb_matrix.shape[0], \n",
    "                           emb_matrix.shape[1], \n",
    "                           input_length=seqlen, \n",
    "                          trainable=False,\n",
    "                           weights=[emb_matrix],\n",
    "                           name=\"embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemb = L_emb\n",
    "emb_x = Lemb(input_x)\n",
    "emb_y = Lemb(input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lconcat = K.layers.Concatenate()\n",
    "concat_x = Lconcat([emb_x, resh_xu])\n",
    "concat_y = Lconcat([emb_y, resh_yu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def realdot(a,b):\n",
    "    d = K.layers.multiply([a, b])\n",
    "    return B.sum(d, axis=-1)\n",
    "\n",
    "def dotlayer(x):\n",
    "    a,b = x\n",
    "    return realdot(a,b)\n",
    "\n",
    "def cossim(x):\n",
    "    a,b = x\n",
    "    p = realdot(a,b)\n",
    "    an = B.sqrt(realdot(a,a) + B.epsilon())\n",
    "    bn = B.sqrt(realdot(b,b) + B.epsilon())\n",
    "    return tf.divide(p, K.layers.multiply([an, bn]) + B.epsilon())\n",
    "\n",
    "def fexpander(x):\n",
    "    return B.expand_dims(x)\n",
    "L_expander = Lambda(fexpander, name=\"expander\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Operands could not be broadcast together with shapes (128, 301) (256, 301)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-65a20c119bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mL_cossim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcossim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine_similarity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mprox1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_cossim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconcat_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprox1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_expander\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprox1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprox2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_cossim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconcat_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-46ac0f44251b>\u001b[0m in \u001b[0;36mcossim\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcossim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrealdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0man\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrealdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrealdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-46ac0f44251b>\u001b[0m in \u001b[0;36mrealdot\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrealdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdotlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(inputs, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwise\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \"\"\"\n\u001b[0;32m--> 588\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_elemwise_op_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# If the inputs have different ranks, we have to reshape them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# to make them broadcastable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36m_compute_elemwise_op_output_shape\u001b[0;34m(self, shape1, shape2)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     raise ValueError('Operands could not be broadcast '\n\u001b[1;32m     60\u001b[0m                                      \u001b[0;34m'together with shapes '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                      str(shape1) + ' ' + str(shape2))\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0moutput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operands could not be broadcast together with shapes (128, 301) (256, 301)"
     ]
    }
   ],
   "source": [
    "lstm =  K.layers.CuDNNLSTM(256, return_sequences=True, name='gru1')(concat_x)\n",
    "print(lstm.shape)\n",
    "#lstm = K.layers.BatchNormalization()(lstm)\n",
    "#lstm =  K.layers.CuDNNLSTM(128, return_sequences=True, name='lstm2')(lstm)\n",
    "#lstm = K.layers.CuDNNGRU(256, return_sequences=True, name='gru2')(lstm)\n",
    "#lstm = K.layers.BatchNormalization()(lstm)\n",
    "lstm = K.layers.CuDNNLSTM(256, return_sequences=True, name='gru3')(lstm)\n",
    "#lstm = L_expander(lstm)\n",
    "dense = K.layers.BatchNormalization()(lstm)\n",
    "\n",
    "def candidate_gen(inp, layername):\n",
    "    out = K.layers.Dense(dim, activation=\"relu\", name=layername+\"_1\")(inp)\n",
    "    out = K.layers.BatchNormalization()(out)\n",
    "#    out = K.layers.Dense(dim, activation=\"relu\", name=layername+\"_2\")(out)\n",
    "#    out = K.layers.BatchNormalization()(out)\n",
    "#    out = K.layers.Dense(dim, activation=\"relu\", name=layername+\"_3\")(out)\n",
    "#    out = K.layers.BatchNormalization()(out)\n",
    "#    out = K.layers.Dense(dim, activation=\"relu\", name=layername+\"_4\")(out)\n",
    "#    out = K.layers.BatchNormalization()(out)\n",
    "    out = K.layers.Dense(dim, activation=\"sigmoid\", name=layername+\"_last\")(out)\n",
    "    out = K.layers.BatchNormalization()(out)\n",
    "    return out\n",
    "\n",
    "out1 = candidate_gen(dense, \"out1\")\n",
    "out2 = candidate_gen(dense, \"out2\")\n",
    "out3 = candidate_gen(dense, \"out3\")\n",
    "\n",
    "allout = K.layers.Concatenate(name=\"concat_out\")([out1, out2, out3, dense])\n",
    "\n",
    "#lstm3 =  K.layers.CuDNNLSTM(256, return_sequences=True, name='lstm3')(allout)\n",
    "\n",
    "probs = K.layers.Dense(dim, activation=\"relu\", name=\"probabilities_intermediate_1\")(allout)\n",
    "probs = K.layers.BatchNormalization()(probs)\n",
    "#probs = K.layers.Dense(dim, activation=\"relu\", name=\"probabilities_intermediate_2\")(probs)\n",
    "#probs = K.layers.BatchNormalization()(probs)\n",
    "probs = K.layers.Dense(3, activation=\"softmax\", name=\"probabilities\")(probs)\n",
    "\n",
    "\n",
    "L_cossim = Lambda(cossim, name=\"cosine_similarity\")\n",
    "prox1 = L_cossim([concat_y, out1])\n",
    "prox1 = L_expander(prox1)\n",
    "prox2 = L_cossim([concat_y, out2])\n",
    "prox2 = L_expander(prox2)\n",
    "prox3 = L_cossim([concat_y, out3])\n",
    "prox3 = L_expander(prox3)\n",
    "\n",
    "allprox = Concatenate(name=\"concat_prox\")([prox1, prox2, prox3])\n",
    "final1 = K.layers.Lambda(dotlayer, name=\"final1\")([probs, allprox])\n",
    "\n",
    "#allones = B.ones(final1.shape, dtype=\"float32\")\n",
    "def final2_fun(x):\n",
    "    return B.abs(1.0 - x)\n",
    "\n",
    "final2 = Lambda(final2_fun, name=\"final2\")(final1)\n",
    "print(probs.shape)\n",
    "print(prox3.shape)\n",
    "print(allout.shape)\n",
    "print(allprox.shape)\n",
    "print(final1.shape)\n",
    "print(final2.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = K.Model(inputs=[input_x, input_xu, input_y, input_yu],\n",
    "               outputs=[final2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_x (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_xu (InputLayer)           (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 128, 300)     3000300     input_x[0][0]                    \n",
      "                                                                 input_y[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 128, 1)       0           input_xu[0][0]                   \n",
      "                                                                 input_yu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 301)     0           embedding[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "                                                                 reshape_1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (CuDNNGRU)                 (None, 128, 256)     429312      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 256)     1024        gru1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (CuDNNGRU)                 (None, 128, 256)     394752      batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 256)     1024        gru2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "gru3 (CuDNNGRU)                 (None, 128, 256)     394752      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 128, 256)     1024        gru3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "out1_1 (Dense)                  (None, 128, 301)     77357       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out2_1 (Dense)                  (None, 128, 301)     77357       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out3_1 (Dense)                  (None, 128, 301)     77357       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 301)     1204        out1_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 128, 301)     1204        out2_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 301)     1204        out3_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out1_last (Dense)               (None, 128, 301)     90902       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out2_last (Dense)               (None, 128, 301)     90902       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out3_last (Dense)               (None, 128, 301)     90902       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_y (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_yu (InputLayer)           (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 301)     1204        out1_last[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 301)     1204        out2_last[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 301)     1204        out3_last[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concat_out (Concatenate)        (None, 128, 1159)    0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "probabilities_intermediate_1 (D (None, 128, 301)     349160      concat_out[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cosine_similarity (Lambda)      (None, 128)          0           concatenate_1[1][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 128, 301)     1204        probabilities_intermediate_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expander (Lambda)               (None, 128, 1)       0           cosine_similarity[0][0]          \n",
      "                                                                 cosine_similarity[1][0]          \n",
      "                                                                 cosine_similarity[2][0]          \n",
      "__________________________________________________________________________________________________\n",
      "probabilities (Dense)           (None, 128, 3)       906         batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concat_prox (Concatenate)       (None, 128, 3)       0           expander[0][0]                   \n",
      "                                                                 expander[1][0]                   \n",
      "                                                                 expander[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "final1 (Lambda)                 (None, 128)          0           probabilities[0][0]              \n",
      "                                                                 concat_prox[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "final2 (Lambda)                 (None, 128)          0           final1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,085,459\n",
      "Trainable params: 2,079,409\n",
      "Non-trainable params: 3,006,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = K.optimizers.Adam(lr=0.01)\n",
    "model.compile(opt, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1792.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2996\n",
      "Epoch 2/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2943\n",
      "Epoch 3/1000000\n",
      "126648/126648 [==============================] - 89s 702us/step - loss: 0.2911\n",
      "Epoch 4/1000000\n",
      "126648/126648 [==============================] - 90s 707us/step - loss: 0.2891\n",
      "Epoch 5/1000000\n",
      "126648/126648 [==============================] - 90s 707us/step - loss: 0.2877\n",
      "Epoch 6/1000000\n",
      "126648/126648 [==============================] - 90s 708us/step - loss: 0.2865\n",
      "Epoch 7/1000000\n",
      "126648/126648 [==============================] - 89s 707us/step - loss: 0.2856\n",
      "Epoch 8/1000000\n",
      "126648/126648 [==============================] - 90s 711us/step - loss: 0.2850\n",
      "Epoch 9/1000000\n",
      "126648/126648 [==============================] - 90s 711us/step - loss: 0.2843\n",
      "Epoch 10/1000000\n",
      "126648/126648 [==============================] - 90s 712us/step - loss: 0.2837\n",
      "Epoch 11/1000000\n",
      "126648/126648 [==============================] - 90s 709us/step - loss: 0.2833\n",
      "Epoch 12/1000000\n",
      "126648/126648 [==============================] - 90s 714us/step - loss: 0.2830\n",
      "Epoch 13/1000000\n",
      "126648/126648 [==============================] - 90s 714us/step - loss: 0.2827\n",
      "Epoch 14/1000000\n",
      "126648/126648 [==============================] - 90s 714us/step - loss: 0.2857\n",
      "Epoch 15/1000000\n",
      "126648/126648 [==============================] - 90s 713us/step - loss: 0.2853\n",
      "Epoch 16/1000000\n",
      "126648/126648 [==============================] - 91s 720us/step - loss: 0.2839\n",
      "Epoch 17/1000000\n",
      "126648/126648 [==============================] - 91s 721us/step - loss: 0.2831\n",
      "Epoch 18/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2825\n",
      "Epoch 19/1000000\n",
      "126648/126648 [==============================] - 92s 723us/step - loss: 0.2821\n",
      "Epoch 20/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2824\n",
      "Epoch 21/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.2817\n",
      "Epoch 22/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2813\n",
      "Epoch 23/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2814\n",
      "Epoch 24/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3484\n",
      "Epoch 25/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.3455\n",
      "Epoch 26/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3387\n",
      "Epoch 27/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3339\n",
      "Epoch 28/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3324\n",
      "Epoch 29/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3290\n",
      "Epoch 30/1000000\n",
      "126648/126648 [==============================] - 91s 715us/step - loss: 0.3265\n",
      "Epoch 31/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3245\n",
      "Epoch 32/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3226\n",
      "Epoch 33/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3235\n",
      "Epoch 34/1000000\n",
      "126648/126648 [==============================] - 90s 714us/step - loss: 0.3208\n",
      "Epoch 35/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3189\n",
      "Epoch 36/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3175\n",
      "Epoch 37/1000000\n",
      "126648/126648 [==============================] - 91s 715us/step - loss: 0.3161\n",
      "Epoch 38/1000000\n",
      "126648/126648 [==============================] - 91s 715us/step - loss: 0.3150\n",
      "Epoch 39/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3138\n",
      "Epoch 40/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3126\n",
      "Epoch 41/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3115\n",
      "Epoch 42/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.3105\n",
      "Epoch 43/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3096\n",
      "Epoch 44/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.3087\n",
      "Epoch 45/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3079\n",
      "Epoch 46/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3071\n",
      "Epoch 47/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3064\n",
      "Epoch 48/1000000\n",
      "126648/126648 [==============================] - 91s 716us/step - loss: 0.3057\n",
      "Epoch 49/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3050\n",
      "Epoch 50/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.3044\n",
      "Epoch 51/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.3038\n",
      "Epoch 52/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.3033\n",
      "Epoch 53/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.3027\n",
      "Epoch 54/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.3022\n",
      "Epoch 55/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.3018\n",
      "Epoch 56/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.3013\n",
      "Epoch 57/1000000\n",
      "126648/126648 [==============================] - 91s 720us/step - loss: 0.3009\n",
      "Epoch 58/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.3004\n",
      "Epoch 59/1000000\n",
      "126648/126648 [==============================] - 91s 722us/step - loss: 0.3000\n",
      "Epoch 60/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2996\n",
      "Epoch 61/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.2993\n",
      "Epoch 62/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2989\n",
      "Epoch 63/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2986\n",
      "Epoch 64/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2983\n",
      "Epoch 65/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2981\n",
      "Epoch 66/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2978\n",
      "Epoch 67/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2975\n",
      "Epoch 68/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2972\n",
      "Epoch 69/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.2970\n",
      "Epoch 70/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2968\n",
      "Epoch 71/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2967\n",
      "Epoch 72/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2964\n",
      "Epoch 73/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.2971\n",
      "Epoch 74/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2966\n",
      "Epoch 75/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.2961\n",
      "Epoch 76/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2958\n",
      "Epoch 77/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.2956\n",
      "Epoch 78/1000000\n",
      "126648/126648 [==============================] - 91s 717us/step - loss: 0.2955\n",
      "Epoch 79/1000000\n",
      "126648/126648 [==============================] - 91s 720us/step - loss: 0.2952\n",
      "Epoch 80/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2952\n",
      "Epoch 81/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2951\n",
      "Epoch 82/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2949\n",
      "Epoch 83/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.2947\n",
      "Epoch 84/1000000\n",
      "126648/126648 [==============================] - 91s 718us/step - loss: 0.2946\n",
      "Epoch 85/1000000\n",
      "126648/126648 [==============================] - 91s 719us/step - loss: 0.2946\n",
      "Epoch 86/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3376\n",
      "Epoch 87/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3406\n",
      "Epoch 88/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3321\n",
      "Epoch 89/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3282\n",
      "Epoch 90/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3257\n",
      "Epoch 91/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3235\n",
      "Epoch 92/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3218\n",
      "Epoch 93/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3204\n",
      "Epoch 94/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3196\n",
      "Epoch 95/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3185\n",
      "Epoch 96/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3177\n",
      "Epoch 97/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3170\n",
      "Epoch 98/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3162\n",
      "Epoch 99/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3158\n",
      "Epoch 100/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3152\n",
      "Epoch 101/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3146\n",
      "Epoch 102/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3143\n",
      "Epoch 103/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3139\n",
      "Epoch 104/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3134\n",
      "Epoch 105/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3143\n",
      "Epoch 106/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3128\n",
      "Epoch 107/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3124\n",
      "Epoch 108/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3119\n",
      "Epoch 109/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3116\n",
      "Epoch 110/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3111\n",
      "Epoch 111/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3107\n",
      "Epoch 112/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3103\n",
      "Epoch 113/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3100\n",
      "Epoch 114/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3098\n",
      "Epoch 115/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3094\n",
      "Epoch 116/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3091\n",
      "Epoch 117/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.3088\n",
      "Epoch 118/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3087\n",
      "Epoch 119/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3084\n",
      "Epoch 120/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3082\n",
      "Epoch 121/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3079\n",
      "Epoch 122/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3077\n",
      "Epoch 123/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.3076\n",
      "Epoch 124/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3073\n",
      "Epoch 125/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3072\n",
      "Epoch 126/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3070\n",
      "Epoch 127/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3067\n",
      "Epoch 128/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3065\n",
      "Epoch 129/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3064\n",
      "Epoch 130/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3063\n",
      "Epoch 131/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3060\n",
      "Epoch 132/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3061\n",
      "Epoch 133/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3056\n",
      "Epoch 134/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3054\n",
      "Epoch 135/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3052\n",
      "Epoch 136/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3049\n",
      "Epoch 137/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3048\n",
      "Epoch 138/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3047\n",
      "Epoch 139/1000000\n",
      "126648/126648 [==============================] - 89s 702us/step - loss: 0.3077\n",
      "Epoch 140/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.3105\n",
      "Epoch 141/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3091\n",
      "Epoch 142/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3093\n",
      "Epoch 143/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3075\n",
      "Epoch 144/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3066\n",
      "Epoch 145/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3060\n",
      "Epoch 146/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3055\n",
      "Epoch 147/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3051\n",
      "Epoch 148/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3048\n",
      "Epoch 149/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3045\n",
      "Epoch 150/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3041\n",
      "Epoch 151/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3037\n",
      "Epoch 152/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3035\n",
      "Epoch 153/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3034\n",
      "Epoch 154/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3031\n",
      "Epoch 155/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3028\n",
      "Epoch 156/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3026\n",
      "Epoch 157/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3025\n",
      "Epoch 158/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3022\n",
      "Epoch 159/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3021\n",
      "Epoch 160/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3019\n",
      "Epoch 161/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3017\n",
      "Epoch 162/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3016\n",
      "Epoch 163/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3015\n",
      "Epoch 164/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3014\n",
      "Epoch 165/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3013\n",
      "Epoch 166/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3011\n",
      "Epoch 167/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3010\n",
      "Epoch 168/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3008\n",
      "Epoch 169/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3007\n",
      "Epoch 170/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3006\n",
      "Epoch 171/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3004\n",
      "Epoch 172/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3003\n",
      "Epoch 173/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3002\n",
      "Epoch 174/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3001\n",
      "Epoch 175/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2999\n",
      "Epoch 176/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2999\n",
      "Epoch 177/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2997\n",
      "Epoch 178/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2997\n",
      "Epoch 179/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2995\n",
      "Epoch 180/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2996\n",
      "Epoch 181/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2997\n",
      "Epoch 182/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2996\n",
      "Epoch 183/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2994\n",
      "Epoch 184/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2992\n",
      "Epoch 185/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2991\n",
      "Epoch 186/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2990\n",
      "Epoch 187/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2989\n",
      "Epoch 188/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2989\n",
      "Epoch 189/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2987\n",
      "Epoch 190/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2986\n",
      "Epoch 191/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2986\n",
      "Epoch 192/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2985\n",
      "Epoch 193/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2985\n",
      "Epoch 194/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2986\n",
      "Epoch 195/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2985\n",
      "Epoch 196/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2983\n",
      "Epoch 197/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2983\n",
      "Epoch 198/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2981\n",
      "Epoch 199/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2980\n",
      "Epoch 200/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2980\n",
      "Epoch 201/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2980\n",
      "Epoch 202/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2979\n",
      "Epoch 203/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2977\n",
      "Epoch 204/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2977\n",
      "Epoch 205/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2976\n",
      "Epoch 206/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2977\n",
      "Epoch 207/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2976\n",
      "Epoch 208/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2974\n",
      "Epoch 209/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2974\n",
      "Epoch 210/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2974\n",
      "Epoch 211/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2973\n",
      "Epoch 212/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2973\n",
      "Epoch 213/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2973\n",
      "Epoch 214/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2971\n",
      "Epoch 215/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2971\n",
      "Epoch 216/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2971\n",
      "Epoch 217/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2970\n",
      "Epoch 218/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2971\n",
      "Epoch 219/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2970\n",
      "Epoch 220/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2969\n",
      "Epoch 221/1000000\n",
      "126648/126648 [==============================] - 89s 702us/step - loss: 0.2980\n",
      "Epoch 222/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2978\n",
      "Epoch 223/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2974\n",
      "Epoch 224/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2973\n",
      "Epoch 225/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2972\n",
      "Epoch 226/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2972\n",
      "Epoch 227/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2972\n",
      "Epoch 228/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2969\n",
      "Epoch 229/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2969\n",
      "Epoch 230/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2971\n",
      "Epoch 231/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2968\n",
      "Epoch 232/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2967\n",
      "Epoch 233/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2967\n",
      "Epoch 234/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2966\n",
      "Epoch 235/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2966\n",
      "Epoch 236/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2966\n",
      "Epoch 237/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2965\n",
      "Epoch 238/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2965\n",
      "Epoch 239/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2965\n",
      "Epoch 240/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2965\n",
      "Epoch 241/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2964\n",
      "Epoch 242/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2963\n",
      "Epoch 243/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2963\n",
      "Epoch 244/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2962\n",
      "Epoch 245/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2962\n",
      "Epoch 246/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2961\n",
      "Epoch 247/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2962\n",
      "Epoch 248/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2961\n",
      "Epoch 249/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2960\n",
      "Epoch 250/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2960\n",
      "Epoch 251/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2960\n",
      "Epoch 252/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2959\n",
      "Epoch 253/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2960\n",
      "Epoch 254/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2959\n",
      "Epoch 255/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2958\n",
      "Epoch 256/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2958\n",
      "Epoch 257/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2958\n",
      "Epoch 258/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2958\n",
      "Epoch 259/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2957\n",
      "Epoch 260/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2958\n",
      "Epoch 261/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2957\n",
      "Epoch 262/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2959\n",
      "Epoch 263/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2958\n",
      "Epoch 264/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2957\n",
      "Epoch 265/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2956\n",
      "Epoch 266/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2956\n",
      "Epoch 267/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2957\n",
      "Epoch 268/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2955\n",
      "Epoch 269/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2955\n",
      "Epoch 270/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2956\n",
      "Epoch 271/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2955\n",
      "Epoch 272/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2955\n",
      "Epoch 273/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2954\n",
      "Epoch 274/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2954\n",
      "Epoch 275/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2954\n",
      "Epoch 276/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2954\n",
      "Epoch 277/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2953\n",
      "Epoch 278/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2953\n",
      "Epoch 279/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2954\n",
      "Epoch 280/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2953\n",
      "Epoch 281/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2953\n",
      "Epoch 282/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2952\n",
      "Epoch 283/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2954\n",
      "Epoch 284/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2958\n",
      "Epoch 285/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2953\n",
      "Epoch 286/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2953\n",
      "Epoch 287/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2953\n",
      "Epoch 288/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2952\n",
      "Epoch 289/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2951\n",
      "Epoch 290/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2950\n",
      "Epoch 291/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2950\n",
      "Epoch 292/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2967\n",
      "Epoch 293/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2952\n",
      "Epoch 294/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2951\n",
      "Epoch 295/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2951\n",
      "Epoch 296/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2950\n",
      "Epoch 297/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2950\n",
      "Epoch 298/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2951\n",
      "Epoch 299/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.3023\n",
      "Epoch 300/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3105\n",
      "Epoch 301/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3059\n",
      "Epoch 302/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.3044\n",
      "Epoch 303/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3035\n",
      "Epoch 304/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3027\n",
      "Epoch 305/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3022\n",
      "Epoch 306/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3018\n",
      "Epoch 307/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3014\n",
      "Epoch 308/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.3011\n",
      "Epoch 309/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3009\n",
      "Epoch 310/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3006\n",
      "Epoch 311/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3004\n",
      "Epoch 312/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3002\n",
      "Epoch 313/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.3000\n",
      "Epoch 314/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2999\n",
      "Epoch 315/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2999\n",
      "Epoch 316/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2996\n",
      "Epoch 317/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2996\n",
      "Epoch 318/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2994\n",
      "Epoch 319/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2993\n",
      "Epoch 320/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2991\n",
      "Epoch 321/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2990\n",
      "Epoch 322/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2989\n",
      "Epoch 323/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2988\n",
      "Epoch 324/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2988\n",
      "Epoch 325/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2987\n",
      "Epoch 326/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2986\n",
      "Epoch 327/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2985\n",
      "Epoch 328/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2984\n",
      "Epoch 329/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2984\n",
      "Epoch 330/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2983\n",
      "Epoch 331/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2982\n",
      "Epoch 332/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2982\n",
      "Epoch 333/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2981\n",
      "Epoch 334/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2980\n",
      "Epoch 335/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2980\n",
      "Epoch 336/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2981\n",
      "Epoch 337/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2978\n",
      "Epoch 338/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2978\n",
      "Epoch 339/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2977\n",
      "Epoch 340/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2976\n",
      "Epoch 341/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2976\n",
      "Epoch 342/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2975\n",
      "Epoch 343/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2975\n",
      "Epoch 344/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2975\n",
      "Epoch 345/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2974\n",
      "Epoch 346/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2973\n",
      "Epoch 347/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2973\n",
      "Epoch 348/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2972\n",
      "Epoch 349/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2972\n",
      "Epoch 350/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2972\n",
      "Epoch 351/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2972\n",
      "Epoch 352/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2971\n",
      "Epoch 353/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2970\n",
      "Epoch 354/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2970\n",
      "Epoch 355/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2970\n",
      "Epoch 356/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2969\n",
      "Epoch 357/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2969\n",
      "Epoch 358/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2969\n",
      "Epoch 359/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2969\n",
      "Epoch 360/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2968\n",
      "Epoch 361/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2968\n",
      "Epoch 362/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2968\n",
      "Epoch 363/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2968\n",
      "Epoch 364/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2967\n",
      "Epoch 365/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2966\n",
      "Epoch 366/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2966\n",
      "Epoch 367/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2966\n",
      "Epoch 368/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2966\n",
      "Epoch 369/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2965\n",
      "Epoch 370/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2965\n",
      "Epoch 371/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2964\n",
      "Epoch 372/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2964\n",
      "Epoch 373/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2963\n",
      "Epoch 374/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2964\n",
      "Epoch 375/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2963\n",
      "Epoch 376/1000000\n",
      "126648/126648 [==============================] - 88s 699us/step - loss: 0.2963\n",
      "Epoch 377/1000000\n",
      "126648/126648 [==============================] - 89s 701us/step - loss: 0.2963\n",
      "Epoch 378/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2963\n",
      "Epoch 379/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2963\n",
      "Epoch 380/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2962\n",
      "Epoch 381/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2962\n",
      "Epoch 382/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2962\n",
      "Epoch 383/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2961\n",
      "Epoch 384/1000000\n",
      "126648/126648 [==============================] - 89s 704us/step - loss: 0.2962\n",
      "Epoch 385/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2961\n",
      "Epoch 386/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2961\n",
      "Epoch 387/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2960\n",
      "Epoch 388/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2960\n",
      "Epoch 389/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2960\n",
      "Epoch 390/1000000\n",
      "126648/126648 [==============================] - 89s 700us/step - loss: 0.2959\n",
      "Epoch 391/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2959\n",
      "Epoch 392/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2959\n",
      "Epoch 393/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2958\n",
      "Epoch 394/1000000\n",
      "126648/126648 [==============================] - 89s 699us/step - loss: 0.2959\n",
      "Epoch 395/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2962\n",
      "Epoch 396/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2959\n",
      "Epoch 397/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2959\n",
      "Epoch 398/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2959\n",
      "Epoch 399/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2958\n",
      "Epoch 400/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2957\n",
      "Epoch 401/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2957\n",
      "Epoch 402/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2956\n",
      "Epoch 403/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2956\n",
      "Epoch 404/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2956\n",
      "Epoch 405/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2956\n",
      "Epoch 406/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2955\n",
      "Epoch 407/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2955\n",
      "Epoch 408/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2955\n",
      "Epoch 409/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2955\n",
      "Epoch 410/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2955\n",
      "Epoch 411/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2955\n",
      "Epoch 412/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2954\n",
      "Epoch 413/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2954\n",
      "Epoch 414/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2953\n",
      "Epoch 415/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2953\n",
      "Epoch 416/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2954\n",
      "Epoch 417/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2953\n",
      "Epoch 418/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2952\n",
      "Epoch 419/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2952\n",
      "Epoch 420/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2952\n",
      "Epoch 421/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2953\n",
      "Epoch 422/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2951\n",
      "Epoch 423/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2952\n",
      "Epoch 424/1000000\n",
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.2951\n",
      "Epoch 425/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2952\n",
      "Epoch 426/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2951\n",
      "Epoch 427/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2951\n",
      "Epoch 428/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2952\n",
      "Epoch 429/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2951\n",
      "Epoch 430/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2950\n",
      "Epoch 431/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.2951\n",
      "Epoch 432/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.2951\n",
      "Epoch 433/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2950\n",
      "Epoch 434/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2950\n",
      "Epoch 435/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2949\n",
      "Epoch 436/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.2949\n",
      "Epoch 437/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.2950\n",
      "Epoch 438/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.2949\n",
      "Epoch 439/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2952\n",
      "Epoch 440/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2998\n",
      "Epoch 441/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3096\n",
      "Epoch 442/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3369\n",
      "Epoch 443/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3283\n",
      "Epoch 444/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3245\n",
      "Epoch 445/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3226\n",
      "Epoch 446/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3212\n",
      "Epoch 447/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3198\n",
      "Epoch 448/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3185\n",
      "Epoch 449/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3171\n",
      "Epoch 450/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3163\n",
      "Epoch 451/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3168\n",
      "Epoch 452/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3154\n",
      "Epoch 453/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3145\n",
      "Epoch 454/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3136\n",
      "Epoch 455/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3129\n",
      "Epoch 456/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3126\n",
      "Epoch 457/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3121\n",
      "Epoch 458/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3116\n",
      "Epoch 459/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3112\n",
      "Epoch 460/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3108\n",
      "Epoch 461/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3110\n",
      "Epoch 462/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3097\n",
      "Epoch 463/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3086\n",
      "Epoch 464/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3081\n",
      "Epoch 465/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3074\n",
      "Epoch 466/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3067\n",
      "Epoch 467/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3061\n",
      "Epoch 468/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3058\n",
      "Epoch 469/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3056\n",
      "Epoch 470/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3053\n",
      "Epoch 471/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3048\n",
      "Epoch 472/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3039\n",
      "Epoch 473/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3037\n",
      "Epoch 474/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3035\n",
      "Epoch 475/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3032\n",
      "Epoch 476/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3030\n",
      "Epoch 477/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3028\n",
      "Epoch 478/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3026\n",
      "Epoch 479/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3024\n",
      "Epoch 480/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3023\n",
      "Epoch 481/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3021\n",
      "Epoch 482/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3020\n",
      "Epoch 483/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3020\n",
      "Epoch 484/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3019\n",
      "Epoch 485/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3019\n",
      "Epoch 486/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3017\n",
      "Epoch 487/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3016\n",
      "Epoch 488/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3015\n",
      "Epoch 489/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3014\n",
      "Epoch 490/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3012\n",
      "Epoch 491/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3012\n",
      "Epoch 492/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3011\n",
      "Epoch 493/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3011\n",
      "Epoch 494/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3010\n",
      "Epoch 495/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3009\n",
      "Epoch 496/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3008\n",
      "Epoch 497/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3008\n",
      "Epoch 498/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3007\n",
      "Epoch 499/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3006\n",
      "Epoch 500/1000000\n",
      "126648/126648 [==============================] - 88s 697us/step - loss: 0.3005\n",
      "Epoch 501/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3005\n",
      "Epoch 502/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3004\n",
      "Epoch 503/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.3004\n",
      "Epoch 504/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3007\n",
      "Epoch 505/1000000\n",
      "126648/126648 [==============================] - 88s 696us/step - loss: 0.3005\n",
      "Epoch 506/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126648/126648 [==============================] - 88s 698us/step - loss: 0.3003\n",
      "Epoch 507/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3004\n",
      "Epoch 508/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3003\n",
      "Epoch 509/1000000\n",
      "126648/126648 [==============================] - 88s 692us/step - loss: 0.3001\n",
      "Epoch 510/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3001\n",
      "Epoch 511/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.3000\n",
      "Epoch 512/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3000\n",
      "Epoch 513/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.3000\n",
      "Epoch 514/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.2999\n",
      "Epoch 515/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.2998\n",
      "Epoch 516/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2997\n",
      "Epoch 517/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.2996\n",
      "Epoch 518/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.2997\n",
      "Epoch 519/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.2996\n",
      "Epoch 520/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2996\n",
      "Epoch 521/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2995\n",
      "Epoch 522/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.2995\n",
      "Epoch 523/1000000\n",
      "126648/126648 [==============================] - 88s 693us/step - loss: 0.2995\n",
      "Epoch 524/1000000\n",
      "126648/126648 [==============================] - 88s 695us/step - loss: 0.2995\n",
      "Epoch 525/1000000\n",
      "126648/126648 [==============================] - 88s 694us/step - loss: 0.2993\n",
      "Epoch 526/1000000\n",
      "126648/126648 [==============================] - 90s 713us/step - loss: 0.2993\n",
      "Epoch 527/1000000\n",
      "  5120/126648 [>.............................] - ETA: 1:29 - loss: 0.3017"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fdb18e9a05a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mYfake\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m         return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1377\u001b[0;31m             self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([X, Xu, Y, Yu], [Yfake], batch_size=512, epochs=1000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = K.Model([input_x, input_xu], [out1, out2, out3, probs])\n",
    "predictor.compile(opt, loss=\"mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "norm_emb_matrix =  pp.normalize(Lemb.get_weights()[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126648"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as the exhibition as the exhibition as the exhibition as the exhibition as the exhibition as the exhibition as the exhibition as the exhibition as the art well s art as the exhibition as the exhibition as the art especially exhibition as the exhibition as the art especially exhibition as the exhibition as the art especially the exhibition as the exhibition as the art especially exhibition as the exhibition as the art especially the exhibition as the art especially the exhibition as the exhibition as the exhibition as the art especially the art especially the exhibition as the art especially as the art especially the exhibition as the art especially as the art especially the exhibition as the art especially the exhibition as the art especially the exhibition |                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                    a 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                 well 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                    s 1.0000 |                    - 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                 work 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |           particular 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |           particular 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |           particular 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |           particular 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |           particular 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |           particular 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |           particular 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |           particular 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                   as 1.0000 |                 well 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|                  art 1.0000 |                  one 0.0000 |                    , 0.0000 |\n",
      "|           especially 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|                  the 1.0000 |                 that 0.0000 |                    , 0.0000 |\n",
      "|           exhibition 1.0000 |                  one 0.0000 |                    , 0.0000 |\n"
     ]
    }
   ],
   "source": [
    "i = 10919\n",
    "gen = X[i].tolist()\n",
    "genu = Xu[i].tolist()\n",
    "\n",
    "tX = np.zeros((1, seqlen), dtype=\"int32\")\n",
    "tXu = np.zeros((1, seqlen), dtype=floatx)\n",
    "results = []\n",
    "\n",
    "def vec2word(x):\n",
    "    we = np.transpose(pp.normalize(np.reshape(x[0, -1, :300], (1,300))))\n",
    "    idx = np.argmax(np.matmul(norm_emb_matrix, we))\n",
    "    return idx\n",
    "\n",
    "iterations = 128\n",
    "\n",
    "allcandidates = []\n",
    "for j in range(iterations):\n",
    "    tX[0,:] = np.array(gen[-seqlen:], \"int32\")\n",
    "    tXu[0,:] = np.array(genu[-seqlen:], floatx)\n",
    "    #print(tX)\n",
    "    xv, xuv = 0, 0.0\n",
    "    yhat = predictor.predict([tX, tXu])\n",
    "    \n",
    "    #for qq in yhat:\n",
    "    #    print(qq.shape)\n",
    "    #print(yhat)\n",
    "    #break\n",
    "    vecid = np.random.choice(3, p=yhat[3][0,0])\n",
    "    z = yhat[vecid]\n",
    "    \n",
    "    \n",
    "    candidates=[]\n",
    "    for u,c in enumerate(yhat[:3]):\n",
    "        cid = vec2word(c)\n",
    "        candidates.append((words[cid], yhat[3][0,0,u]))\n",
    "    allcandidates.append(candidates)\n",
    "#    print(candidates)\n",
    "    \n",
    "    if False:# z[0, -1, -1] > 0.9:\n",
    "        xuv = 1.0\n",
    "        results.append(\"<UNK>\")\n",
    "    else:\n",
    "        we = np.transpose(pp.normalize(np.reshape(z[0, -1, :300], (1,300))))\n",
    "#        print(np.linalg.norm(we))\n",
    "#        scores = np.matmul(norm_emb_matrix, we)\n",
    "#        print(scores)\n",
    "#        print(np.min(scores))\n",
    "#        break\n",
    "        xv = np.argmax(np.matmul(norm_emb_matrix, we))\n",
    "        if xv:\n",
    "            results.append(words[xv])\n",
    "        else:\n",
    "            results.append(\"<EMPTY>\")\n",
    "    gen.append(xv)\n",
    "    genu.append(xuv)\n",
    "    sys.stdout.write(results[-1]+\" \")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "for cc in allcandidates:\n",
    "    print(\"| %20s %.4f | %20s %.4f | %20s %.4f |\" %\n",
    "         (cc[0][0], cc[0][1],\n",
    "         cc[1][0], cc[1][1],\n",
    "         cc[2][0], cc[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.9707959 11.670498\n"
     ]
    }
   ],
   "source": [
    "print(np.min(z), np.max(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_emb_matrix =  preprocessing.normalize(emb_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "zn = preprocessing.normalize(z[0,:], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = zn[-1, :300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.matmul(norm_emb_matrix, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(scores)\n",
    "print(idx)\n",
    "word = words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
