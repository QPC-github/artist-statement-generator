{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K\n",
    "import os.path\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from artstat import util\n",
    "import numpy as np\n",
    "\n",
    "floatx = \"float32\"\n",
    "K.backend.set_floatx(floatx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, Reshape, Concatenate, CuDNNLSTM, Lambda\n",
    "from keras import backend as B\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'artstat.util' from '../src/artstat/util.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/home/pmilovanov/hg/my/data/gallery-pr/all\"\n",
    "\n",
    "path_train = os.path.join(datadir, \"train\")\n",
    "path_test = os.path.join(datadir, \"test\")\n",
    "\n",
    "glove = \"/home/pmilovanov/data/glove/glove.6B.50d.txt\"\n",
    "glove_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocab = util.load_vocab(\"../vocab.txt\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = util.load_embeddings(vocab, glove_dim, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez(\"embmat_100K.npz\", emb_matrix=emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 50) 100000\n"
     ]
    }
   ],
   "source": [
    "print(emb_matrix.shape, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 22860/22860 [00:33<00:00, 690.48it/s]\n"
     ]
    }
   ],
   "source": [
    "imp.reload(util)\n",
    "\n",
    "seqlen = 128\n",
    "#X, Y, Xu, Yu = util.load_data_sequences(path_train, vocab, seqlen, stride)\n",
    "\n",
    "X, Xu = util.load_data(path_train, vocab, seqlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = K.Input((seqlen,), dtype=\"int32\", name=\"input_x\")\n",
    "input_xu = K.Input((seqlen,), dtype=floatx, name=\"input_xu\")\n",
    "#input_y = K.Input((seqlen,), dtype=\"int32\", name=\"input_y\")\n",
    "#input_yu = K.Input((seqlen,), dtype=floatx, name=\"input_yu\")\n",
    "input_y = K.Input((1,), dtype=\"int32\", name=\"input_y\")\n",
    "input_yu = K.Input((1,), dtype=floatx, name=\"input_yu\")\n",
    "\n",
    "def expander(id):\n",
    "    return Lambda(lambda x: B.expand_dims(x), name=\"expander_\" + id )\n",
    "\n",
    "\n",
    "#reshaper_u  = K.layers.Reshape((seqlen,1))\n",
    "#resh_xu = reshaper_u(input_xu)\n",
    "#resh_yu = reshaper_u(input_yu)\n",
    "\n",
    "resh_xu = expander(\"xu\")(input_xu)\n",
    "resh_yu = expander(\"yu\")(input_yu)\n",
    "\n",
    "dim = emb_matrix.shape[1] + 1\n",
    "\n",
    "L_emb1 = K.layers.Embedding(emb_matrix.shape[0], \n",
    "                           emb_matrix.shape[1], \n",
    "                           input_length=seqlen, \n",
    "                          trainable=False,\n",
    "                           weights=[emb_matrix],\n",
    "                           name=\"embedding1\")\n",
    "\n",
    "\n",
    "L_emb2 = K.layers.Embedding(emb_matrix.shape[0], \n",
    "                           emb_matrix.shape[1], \n",
    "                           input_length=1, \n",
    "                          trainable=False,\n",
    "                           weights=[emb_matrix],\n",
    "                           name=\"embedding2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_x = L_emb1(input_x)\n",
    "emb_y = L_emb2(input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lconcat = K.layers.Concatenate()\n",
    "concat_x = Lconcat([emb_x, resh_xu])\n",
    "concat_y = Concatenate()([emb_y, resh_yu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def realdot(a,b):\n",
    "    d = K.layers.multiply([a, b])\n",
    "    return B.sum(d, axis=-1)\n",
    "\n",
    "def dotlayer(x):\n",
    "    a,b = x\n",
    "    return realdot(a,b)\n",
    "\n",
    "def cossim(x):\n",
    "    a,b = x\n",
    "    p = realdot(a,b)\n",
    "    an = B.sqrt(realdot(a,a) + B.epsilon())\n",
    "    bn = B.sqrt(realdot(b,b) + B.epsilon())\n",
    "    return tf.divide(p, K.layers.multiply([an, bn]) + B.epsilon())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 256)\n",
      "p2 (?, 3)\n",
      "(?, 1)\n",
      "p2 (?, 3)\n",
      "(?, 1)\n",
      "probs (?, 3)\n",
      "(?, 1, 1)\n",
      "(?, 409)\n",
      "(?, 1, 3)\n",
      "final1 (?, 1)\n",
      "final2 (?, 1)\n"
     ]
    }
   ],
   "source": [
    "lstm =  K.layers.CuDNNLSTM(256, return_sequences=True, name='lstm1')(concat_x)\n",
    "print(lstm.shape)\n",
    "lstm = K.layers.BatchNormalization()(lstm)\n",
    "lstm =  K.layers.CuDNNLSTM(256, return_sequences=False, name='lstm2')(lstm)\n",
    "#lstm = K.layers.BatchNormalization()(lstm)\n",
    "#lstm =  K.layers.CuDNNLSTM(128, return_sequences=False, name='lstm3')(lstm)\n",
    "#lstm = K.layers.CuDNNGRU(256, return_sequences=True, name='gru2')(lstm)\n",
    "#lstm = K.layers.BatchNormalization()(lstm)\n",
    "#lstm = K.layers.CuDNNLSTM(256, return_sequences=False, name='gru3')(lstm)\n",
    "#lstm = L_expander(lstm)\n",
    "dense = K.layers.BatchNormalization()(lstm)\n",
    "\n",
    "def candidate_gen(inp, layername):\n",
    "    out = K.layers.Dense(dim, activation=\"relu\", name=layername+\"_1\")(inp)\n",
    "    out = K.layers.BatchNormalization()(out)\n",
    "#    out = K.layers.Dense(dim, activation=\"relu\", name=layername+\"_2\")(out)\n",
    "#    out = K.layers.BatchNormalization()(out)\n",
    "#    out = K.layers.Dense(dim, activation=\"relu\", name=layername+\"_3\")(out)\n",
    "#    out = K.layers.BatchNormalization()(out)\n",
    "    out = K.layers.Dense(dim, activation=\"relu\", name=layername+\"_4\")(out)\n",
    "    out = K.layers.BatchNormalization()(out)\n",
    "    out = K.layers.Dense(dim, activation=\"sigmoid\", name=layername+\"_last\")(out)\n",
    "    out = K.layers.BatchNormalization()(out)\n",
    "    return out\n",
    "\n",
    "out1 = candidate_gen(dense, \"out1\")\n",
    "out2 = candidate_gen(dense, \"out2\")\n",
    "out3 = candidate_gen(dense, \"out3\")\n",
    "\n",
    "L_cossim_expand = expander(\"candidate_cossim\")\n",
    "L_cossim_out = Lambda(cossim, name=\"cossim-out123\")\n",
    "cossim_out_12 = L_cossim_expand(L_cossim_out([out1, out2]))\n",
    "cossim_out_13 = L_cossim_expand(L_cossim_out([out1, out3]))\n",
    "cossim_out_23 = L_cossim_expand(L_cossim_out([out2, out3]))\n",
    "\n",
    "cossim_out_all = Concatenate(name=\"concat_cossim_out_all\")([cossim_out_12,\n",
    "                                                           cossim_out_13,\n",
    "                                                           cossim_out_23])\n",
    "#print(\"cossim_out_all\", cossim_out_all.shape)\n",
    "def candidates_proximity_penalty_fn(x):\n",
    "    threshold = 0.9\n",
    "    coeff = 20.0\n",
    "    penalty = B.exp(coeff * B.maximum(0.0, x - threshold)) - 1.0\n",
    "    penalty = B.max(penalty, axis=-1, keepdims=True)\n",
    "    return penalty\n",
    "    \n",
    "candidates_proximity_penalty = Lambda(candidates_proximity_penalty_fn)(cossim_out_all)\n",
    "\n",
    "allout = K.layers.Concatenate(name=\"concat_out\")([out1, out2, out3, dense])\n",
    "\n",
    "#lstm3 =  K.layers.CuDNNLSTM(256, return_sequences=True, name='lstm3')(allout)\n",
    "\n",
    "#probs = K.layers.Dense(dim, activation=\"relu\", name=\"probabilities_intermediate_1\")(allout)\n",
    "#probs = K.layers.BatchNormalization()(probs)\n",
    "#probs = K.layers.Dense(dim, activation=\"relu\", name=\"probabilities_intermediate_2\")(probs)\n",
    "#probs = K.layers.BatchNormalization()(probs)\n",
    "probs = K.layers.Dense(3, activation=\"softmax\", name=\"probabilities\")(allout)\n",
    "\n",
    "L_cossim = Lambda(cossim, name=\"cosine_similarity\")\n",
    "prox1 = L_cossim([concat_y, out1])\n",
    "prox1 = expander(\"prox1\")(prox1)\n",
    "prox2 = L_cossim([concat_y, out2])\n",
    "prox2 = expander(\"prox2\")(prox2)\n",
    "prox3 = L_cossim([concat_y, out3])\n",
    "prox3 = expander(\"prox3\")(prox3)\n",
    "\n",
    "allprox = Concatenate(name=\"concat_prox\")([prox1, prox2, prox3])\n",
    "final1 = K.layers.Lambda(dotlayer, name=\"final1\")([probs, allprox])\n",
    "\n",
    "def final2_fun(x):\n",
    "    threshold = 0.9\n",
    "    coeff= 20.0\n",
    "    f1, p, cand_prox_pen = x\n",
    "    probs_penalty = B.exp(coeff * B.maximum(0.0, p - threshold)) - 1.0\n",
    "    print(\"p2\", probs_penalty.shape)\n",
    "    probs_penalty = B.max(probs_penalty, axis=-1, keepdims=True)\n",
    "    print(probs_penalty.shape)\n",
    "    return B.abs(1.0 - f1) + probs_penalty + cand_prox_pen\n",
    "\n",
    "final2 = Lambda(final2_fun, name=\"final2\")([final1, \n",
    "                                            probs, candidates_proximity_penalty])\n",
    "print(\"probs\", probs.shape)\n",
    "print(prox3.shape)\n",
    "print(allout.shape)\n",
    "print(allprox.shape)\n",
    "print(\"final1\", final1.shape)\n",
    "print(\"final2\", final2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = K.Model(inputs=[input_x, input_xu, input_y, input_yu],\n",
    "               outputs=[final2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_x (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_xu (InputLayer)           (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding1 (Embedding)          (None, 128, 50)      5000050     input_x[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "expander_xu (Lambda)            (None, 128, 1)       0           input_xu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 51)      0           embedding1[0][0]                 \n",
      "                                                                 expander_xu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (CuDNNLSTM)               (None, 128, 256)     316416      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 256)     1024        lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (CuDNNLSTM)               (None, 256)          526336      batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 256)          1024        lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out1_1 (Dense)                  (None, 51)           13107       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out2_1 (Dense)                  (None, 51)           13107       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out3_1 (Dense)                  (None, 51)           13107       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 51)           204         out1_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 51)           204         out2_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 51)           204         out3_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out1_4 (Dense)                  (None, 51)           2652        batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out2_4 (Dense)                  (None, 51)           2652        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out3_4 (Dense)                  (None, 51)           2652        batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 51)           204         out1_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 51)           204         out2_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 51)           204         out3_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_y (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_yu (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "out1_last (Dense)               (None, 51)           2652        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out2_last (Dense)               (None, 51)           2652        batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out3_last (Dense)               (None, 51)           2652        batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding2 (Embedding)          (None, 1, 50)        5000050     input_y[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "expander_yu (Lambda)            (None, 1, 1)         0           input_yu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 51)           204         out1_last[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 51)           204         out2_last[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 51)           204         out3_last[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 51)        0           embedding2[0][0]                 \n",
      "                                                                 expander_yu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cosine_similarity (Lambda)      (None, 1)            0           concatenate_2[0][0]              \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cossim-out123 (Lambda)          (None,)              0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concat_out (Concatenate)        (None, 409)          0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "expander_prox1 (Lambda)         (None, 1, 1)         0           cosine_similarity[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "expander_prox2 (Lambda)         (None, 1, 1)         0           cosine_similarity[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "expander_prox3 (Lambda)         (None, 1, 1)         0           cosine_similarity[2][0]          \n",
      "__________________________________________________________________________________________________\n",
      "expander_candidate_cossim (Lamb (None, 1)            0           cossim-out123[0][0]              \n",
      "                                                                 cossim-out123[1][0]              \n",
      "                                                                 cossim-out123[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "probabilities (Dense)           (None, 3)            1230        concat_out[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concat_prox (Concatenate)       (None, 1, 3)         0           expander_prox1[0][0]             \n",
      "                                                                 expander_prox2[0][0]             \n",
      "                                                                 expander_prox3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concat_cossim_out_all (Concaten (None, 3)            0           expander_candidate_cossim[0][0]  \n",
      "                                                                 expander_candidate_cossim[1][0]  \n",
      "                                                                 expander_candidate_cossim[2][0]  \n",
      "__________________________________________________________________________________________________\n",
      "final1 (Lambda)                 (None, 1)            0           probabilities[0][0]              \n",
      "                                                                 concat_prox[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           concat_cossim_out_all[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "final2 (Lambda)                 (None, 1)            0           final1[0][0]                     \n",
      "                                                                 probabilities[0][0]              \n",
      "                                                                 lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10,903,199\n",
      "Trainable params: 901,157\n",
      "Non-trainable params: 10,002,042\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = K.optimizers.Adam(lr=0.01)\n",
    "model.compile(opt, loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 128) (2048, 128) (2048, 1) (2048, 1) (2048, 1)\n"
     ]
    }
   ],
   "source": [
    "imp.reload(util)\n",
    "batch_size=2048\n",
    "train_seq = util.SpecialSequence(X, Xu, seqlen, batch_size)\n",
    "\n",
    "[bX, bXu, bY, bYu], [bYfake] = train_seq[250]\n",
    "\n",
    "print(bX.shape, bXu.shape, bY.shape, bYu.shape, bYfake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000000\n",
      "66/66 [==============================] - 42s 634ms/step - loss: 1.0167\n",
      "Epoch 2/1000000\n",
      "66/66 [==============================] - 24s 359ms/step - loss: 0.6424\n",
      "Epoch 3/1000000\n",
      "66/66 [==============================] - 24s 361ms/step - loss: 0.5920\n",
      "Epoch 4/1000000\n",
      "66/66 [==============================] - 24s 364ms/step - loss: 0.5432\n",
      "Epoch 5/1000000\n",
      "66/66 [==============================] - 24s 363ms/step - loss: 0.5731\n",
      "Epoch 6/1000000\n",
      " 9/66 [===>..........................] - ETA: 21s - loss: 0.5699"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-081cef3c4615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model.fit([X, Xu, Y, Yu], [Yfake], batch_size=512, epochs=1000000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m         return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1377\u001b[0;31m             self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.fit([X, Xu, Y, Yu], [Yfake], batch_size=512, epochs=1000000)\n",
    "\n",
    "model.fit_generator(train_seq, steps_per_epoch=int(len(train_seq)/(batch_size*seqlen)), epochs=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = K.Model([input_x, input_xu], [out1, out2, out3, probs])\n",
    "predictor.compile(opt, loss=\"mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "norm_emb_matrix =  pp.normalize(L_emb1.get_weights()[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17676455"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[50, 25548, 22, 2, 23113, 816, 3, 692, 6, 496, 6680, 136, 50, 469, 32880, 3, 14700, 136, 56, 7, 14716, 90, 290, 1558, 1, 3354, 136, 1558, 3, 46, 301, 4, 868, 1, 276, 676, 56, 64, 265, 32880, 24, 80, 271, 86, 69, 9576, 1, 57, 1068, 1, 27, 3268, 7343, 8, 3578, 3, 46, 7, 0, 15, 2, 2090, 3, 7437, 238, 90, 1074, 16, 166, 111, 457, 32880, 50, 279, 16, 496, 902, 50, 23, 1068, 3, 744, 85, 469, 9261, 22, 7, 39117, 21, 7, 25552, 13257, 951, 3, 27152, 5, 51153, 4, 2, 1224, 1, 1493, 401, 57, 13, 123, 2, 0, 1, 2, 0, 1, 1262, 1, 17326, 8, 35712, 0, 1, 85260, 3, 6177, 24, 42, 28003, 4, 52218, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "t2s = util.Text2Seq(vocab)\n",
    "\n",
    "text = \"\"\"\n",
    "Lolita, light of my life, fire of my loins. My sin, my soul. Lo-lee-ta: the tip of the tongue taking a trip of three steps down the palate to tap, at three, on the teeth. Lo. Lee. Ta. She was Lo, plain Lo, in the morning, standing four feet ten in one sock. She was Lola in slacks. She was Dolly at school. She was Dolores on the dotted line. But in my arms she was always Lolita. Did she have a precursor? She did, indeed she did. In point of fact, there might have been no Lolita at all had I not loved, one summer, an initial girl-child. In a princedom by the sea. Oh when? About as many years before Lolita was born as my age was that summer. You can always count on a murderer for a fancy prose style. Ladies and gentlemen of the jury, exhibit number one is what the seraphs, the misinformed, simple, noble-winged seraphs, envied. Look at this tangle of thorns.\"\"\"\n",
    "\n",
    "sx, sxu = t2s.toseq(text)\n",
    "\n",
    "if len(sx) > seqlen:\n",
    "    sx = sx[-seqlen:]\n",
    "    sxu = sxu[-seqlen:]\n",
    "\n",
    "print(len(sx))\n",
    "print(sx)\n",
    "print(sxu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde ugm ugm arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde arde spectre ugm ugm ugm ugm reintroduction 4002 ugm ugm ugm reintroduction arde peintre collet arde peintre arde arde peintre arde peintre collet peintre collet peintre collet arde peintre charlatan peintre charlatan peintre arde peintre traitor arde arde traitor traitor traitor arde traitor arde arde traitor traitor traitor traitor arde traitor traitor traitor traitor arde arde traitor traitor traitor traitor arde traitor tyrant etoy arde arde traitor tyrant treason tyrant treason arde arde tyrant tyrant |       arde 1.00 |        5cm 0.00 |      blend 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      makes 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |    quality 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      makes 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |        fit 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 | everything 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      blend 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      blend 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      blend 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      blend 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |    quality 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |    quality 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 | concentrate 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 | concentrate 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 | concentrate 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |    quality 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      blend 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      blend 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 | concentrate 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 | concentrate 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 | concentrate 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |   programs 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |      basic 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |        mix 0.00 |\n",
      "|        ugm 1.00 |        5cm 0.00 |     basics 0.00 |\n",
      "|        ugm 1.00 |        5cm 0.00 |     basics 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |    upgrade 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |       buys 0.00 |\n",
      "|       arde 1.00 |        5cm 0.00 |       buys 0.00 |\n",
      "|       arde 0.99 |        5cm 0.00 | purchasing 0.00 |\n",
      "|       arde 0.98 |        5cm 0.00 |      sells 0.02 |\n",
      "|       arde 0.97 |        5cm 0.00 |      sells 0.03 |\n",
      "|       arde 0.98 |        5cm 0.00 |    product 0.01 |\n",
      "|       arde 0.99 |        5cm 0.00 |    premium 0.01 |\n",
      "|       arde 0.99 |        5cm 0.00 |    premium 0.01 |\n",
      "|       arde 0.99 |        5cm 0.00 |      worth 0.01 |\n",
      "|       arde 0.99 |        5cm 0.00 |     annual 0.01 |\n",
      "|       arde 0.99 |        5cm 0.00 |    expects 0.01 |\n",
      "|       arde 0.99 |        5cm 0.00 |    expects 0.00 |\n",
      "|       arde 0.99 |        5cm 0.00 |    expects 0.00 |\n",
      "|       arde 1.00 | aestheticized 0.00 |    expects 0.00 |\n",
      "|       arde 1.00 |  assiduous 0.00 |    expects 0.00 |\n",
      "|       arde 1.00 |  ceaseless 0.00 |    biggest 0.00 |\n",
      "|       arde 1.00 |  ceaseless 0.00 |    biggest 0.00 |\n",
      "|       arde 1.00 |  ceaseless 0.00 |    biggest 0.00 |\n",
      "|       arde 1.00 |  ceaseless 0.00 |    biggest 0.00 |\n",
      "|       arde 1.00 |  ceaseless 0.00 |    biggest 0.00 |\n",
      "|       arde 0.99 |  assiduous 0.01 |    biggest 0.00 |\n",
      "|       arde 0.98 | ambivalently 0.02 |   prospect 0.00 |\n",
      "|       arde 0.96 | encroaches 0.04 |    biggest 0.00 |\n",
      "|       arde 0.95 | encroaches 0.05 |    biggest 0.00 |\n",
      "|       arde 0.95 | encroaches 0.05 |    biggest 0.00 |\n",
      "|       arde 0.95 | encroaches 0.05 |    biggest 0.00 |\n",
      "|       arde 0.92 | encroaches 0.08 |   prospect 0.00 |\n",
      "|        ugm 0.87 |    spectre 0.13 |      knick 0.00 |\n",
      "|        ugm 0.86 | reintroduction 0.14 |      knick 0.00 |\n",
      "|        ugm 0.86 | reintroduction 0.14 |      knick 0.00 |\n",
      "|        ugm 0.86 | reintroduction 0.14 |      knick 0.00 |\n",
      "|        ugm 0.86 | reintroduction 0.14 |      knick 0.00 |\n",
      "|        ugm 0.86 | reintroduction 0.14 |      knick 0.00 |\n",
      "|       4002 0.85 |     remake 0.15 |      knick 0.00 |\n",
      "|        ugm 0.86 | reintroduction 0.14 |      knick 0.00 |\n",
      "|        ugm 0.86 | reintroduction 0.14 |      knick 0.00 |\n",
      "|        ugm 0.86 | reintroduction 0.14 |      knick 0.00 |\n",
      "|        ugm 0.85 | reintroduction 0.15 |      knick 0.00 |\n",
      "|       arde 0.66 |      haven 0.34 |      knick 0.00 |\n",
      "|       arde 0.28 |    peintre 0.71 |   nordberg 0.01 |\n",
      "|       arde 0.28 |     collet 0.70 |   portends 0.02 |\n",
      "|       arde 0.26 |    peintre 0.73 |   portends 0.01 |\n",
      "|       arde 0.25 |    peintre 0.74 |   nordberg 0.01 |\n",
      "|       arde 0.27 |     collet 0.71 |   portends 0.02 |\n",
      "|       arde 0.25 |    peintre 0.74 |   nordberg 0.01 |\n",
      "|       arde 0.25 |    peintre 0.74 |   nordberg 0.01 |\n",
      "|       arde 0.27 |     collet 0.71 |      knick 0.02 |\n",
      "|       arde 0.25 |    peintre 0.74 |   nordberg 0.01 |\n",
      "|       arde 0.27 |     collet 0.72 |      knick 0.02 |\n",
      "|       arde 0.26 |    peintre 0.73 |   portends 0.01 |\n",
      "|       arde 0.26 |     collet 0.72 |      knick 0.02 |\n",
      "|       arde 0.26 |    peintre 0.73 |   portends 0.01 |\n",
      "|       arde 0.26 |     collet 0.72 |      knick 0.02 |\n",
      "|       arde 0.26 |    peintre 0.73 |   portends 0.01 |\n",
      "|       arde 0.25 |    peintre 0.74 |   nordberg 0.01 |\n",
      "|       arde 0.27 |  charlatan 0.72 |      knick 0.02 |\n",
      "|       arde 0.24 |    peintre 0.75 |   nordberg 0.01 |\n",
      "|       arde 0.27 |  charlatan 0.72 |      knick 0.01 |\n",
      "|       arde 0.24 |    peintre 0.75 |   nordberg 0.01 |\n",
      "|       arde 0.27 |  charlatan 0.72 |      knick 0.01 |\n",
      "|       arde 0.25 |    peintre 0.74 |   nordberg 0.01 |\n",
      "|       arde 0.27 |    traitor 0.71 |      knick 0.01 |\n",
      "|       arde 0.25 |    traitor 0.74 |   portends 0.01 |\n",
      "|       arde 0.25 |    traitor 0.74 |   portends 0.01 |\n",
      "|       arde 0.26 |    traitor 0.74 |   portends 0.01 |\n",
      "|       arde 0.27 |    traitor 0.73 |   portends 0.01 |\n",
      "|       arde 0.27 |    traitor 0.73 |   portends 0.01 |\n",
      "|       arde 0.27 |    traitor 0.72 |   portends 0.01 |\n",
      "|       arde 0.26 |    traitor 0.73 |   portends 0.01 |\n",
      "|       arde 0.29 |    traitor 0.70 |   portends 0.01 |\n",
      "|       arde 0.27 |    traitor 0.73 |   portends 0.01 |\n",
      "|       arde 0.27 |    traitor 0.72 |   portends 0.01 |\n",
      "|       arde 0.30 |    traitor 0.69 |   portends 0.01 |\n",
      "|       arde 0.30 |    traitor 0.69 |   portends 0.01 |\n",
      "|       arde 0.30 |    traitor 0.69 |   portends 0.01 |\n",
      "|       arde 0.31 |    traitor 0.69 |   portends 0.01 |\n",
      "|       arde 0.29 |    traitor 0.71 |   portends 0.01 |\n",
      "|       arde 0.31 |    traitor 0.68 |   portends 0.01 |\n",
      "|       arde 0.31 |    traitor 0.68 |   portends 0.01 |\n",
      "|       arde 0.31 |    traitor 0.68 |   portends 0.01 |\n",
      "|       arde 0.32 |     tyrant 0.68 |   portends 0.01 |\n",
      "|       arde 0.29 |    traitor 0.70 |   portends 0.01 |\n",
      "|       arde 0.29 |    traitor 0.70 |   portends 0.01 |\n",
      "|       arde 0.32 |    traitor 0.67 |   portends 0.01 |\n",
      "|       arde 0.32 |    traitor 0.67 |   portends 0.01 |\n",
      "|       arde 0.32 |    traitor 0.67 |      knick 0.01 |\n",
      "|       arde 0.32 |    traitor 0.67 |      knick 0.01 |\n",
      "|       arde 0.30 |    traitor 0.70 |   portends 0.01 |\n",
      "|       arde 0.32 |     tyrant 0.67 |   portends 0.01 |\n",
      "|       etoy 0.32 |    treason 0.68 |   portends 0.01 |\n",
      "|       arde 0.28 |    treason 0.71 |   portends 0.01 |\n",
      "|       arde 0.30 |    traitor 0.69 |   portends 0.01 |\n",
      "|       arde 0.30 |    traitor 0.69 |   portends 0.01 |\n",
      "|       arde 0.32 |     tyrant 0.67 |   portends 0.01 |\n",
      "|       etoy 0.31 |    treason 0.68 |   portends 0.01 |\n",
      "|       arde 0.34 |     tyrant 0.64 |      slump 0.02 |\n",
      "|       etoy 0.32 |    treason 0.67 |   portends 0.01 |\n",
      "|       arde 0.34 |     tyrant 0.64 |      slump 0.02 |\n",
      "|       arde 0.32 |     tyrant 0.67 |   portends 0.01 |\n",
      "|       arde 0.32 |     tyrant 0.67 |   portends 0.01 |\n",
      "|       arde 0.32 |     tyrant 0.67 |    upsurge 0.01 |\n"
     ]
    }
   ],
   "source": [
    "i = 99999\n",
    "\n",
    "\n",
    "gen = X[i:i+seqlen].tolist()\n",
    "genu = X[i:i+seqlen].tolist()\n",
    "\n",
    "#gen = [0]*(seqlen - len(sx)) + sx\n",
    "#genu = [0]*(seqlen - len(sxu)) + sxu\n",
    "\n",
    "\n",
    "tX = np.zeros((1, seqlen), dtype=\"int32\")\n",
    "tXu = np.zeros((1, seqlen), dtype=floatx)\n",
    "results = []\n",
    "\n",
    "def vec2word(x):\n",
    "    we = np.transpose(pp.normalize(np.reshape(x[0, :glove_dim], (1,glove_dim))))\n",
    "    idx = np.argmax(np.matmul(norm_emb_matrix, we))\n",
    "    return idx\n",
    "\n",
    "iterations = 128\n",
    "\n",
    "allcandidates = []\n",
    "for j in range(iterations):\n",
    "    tX[0,:] = np.array(gen[-seqlen:], \"int32\")\n",
    "    tXu[0,:] = np.array(genu[-seqlen:], floatx)\n",
    "    #print(tX)\n",
    "    xv, xuv = 0, 0.0\n",
    "    yhat = predictor.predict([tX, tXu])\n",
    "    \n",
    "#    print(yhat[0].shape)\n",
    "#    break\n",
    "    \n",
    "    #for qq in yhat:\n",
    "    #    print(qq.shape)\n",
    "    #print(yhat)\n",
    "    #break\n",
    "    vecid = np.random.choice(3, p=yhat[3][0])\n",
    "    z = yhat[vecid]\n",
    "    \n",
    "    \n",
    "    candidates=[]\n",
    "    for u,c in enumerate(yhat[:3]):\n",
    "        cid = vec2word(c)\n",
    "        candidates.append((words[cid], yhat[3][0,u]))\n",
    "    allcandidates.append(candidates)\n",
    "#    print(candidates)\n",
    "    \n",
    "    if False:# z[0, -1, -1] > 0.9:\n",
    "        xuv = 1.0\n",
    "        results.append(\"<UNK>\")\n",
    "    else:\n",
    "#        we = np.transpose(pp.normalize(np.reshape(z[0, :300], (1,300))))\n",
    "#        print(np.linalg.norm(we))\n",
    "#        scores = np.matmul(norm_emb_matrix, we)\n",
    "#        print(scores)\n",
    "#        print(np.min(scores))\n",
    "#        break\n",
    "#        xv = np.argmax(np.matmul(norm_emb_matrix, we))\n",
    "        xv = vec2word(z)\n",
    "        if xv:\n",
    "            results.append(words[xv])\n",
    "        else:\n",
    "            results.append(\"<EMPTY>\")\n",
    "    gen.append(xv)\n",
    "    genu.append(xuv)\n",
    "    sys.stdout.write(results[-1]+\" \")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "for cc in allcandidates:\n",
    "    print(\"| %10s %.2f | %10s %.2f | %10s %.2f |\" %\n",
    "         (cc[0][0], cc[0][1],\n",
    "         cc[1][0], cc[1][1],\n",
    "         cc[2][0], cc[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.9707959 11.670498\n"
     ]
    }
   ],
   "source": [
    "print(np.min(z), np.max(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_emb_matrix =  preprocessing.normalize(emb_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "zn = preprocessing.normalize(z[0,:], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = zn[-1, :300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.matmul(norm_emb_matrix, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(scores)\n",
    "print(idx)\n",
    "word = words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
